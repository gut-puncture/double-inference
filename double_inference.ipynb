{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/double-inference/blob/main/double_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAJU3CLZdGI3",
        "outputId": "90dd82db-87cf-4021-bd80-3ed3b1f77223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "VRAM: 42.5 GB\n",
            "ExperimentConfig(model_path='/content/drive/MyDrive/phi3_3.8B', max_length=2048, temperature=0.7, top_p=0.9, seed=42, pass_type='baseline', second_pass_layers=None, residual_variant='raw', attn_impl='auto', entropy_eps=1e-09, min_vram_gb=10.0)\n"
          ]
        }
      ],
      "source": [
        "### 1️⃣ Setup & Config\n",
        "\n",
        "# Fix version conflicts by installing compatible versions\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.44.0 accelerate datasets==2.18.0\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# All imports consolidated (removed lm-eval to avoid conflicts)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# GPU check\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cpu': raise ValueError('No GPU available')\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f'GPU: {gpu_name}')\n",
        "print(f'VRAM: {vram_gb:.1f} GB')\n",
        "\n",
        "# Config with constants\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    model_path: str = '/content/drive/MyDrive/phi3_3.8B'\n",
        "    max_length: int = 2048\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "    seed: int = 42\n",
        "    pass_type: str = 'baseline'\n",
        "    second_pass_layers: int = None\n",
        "    residual_variant: str = 'raw'\n",
        "    attn_impl: str = 'auto'\n",
        "    entropy_eps: float = 1e-9  # New constant\n",
        "    min_vram_gb: float = 10.0  # New constant\n",
        "\n",
        "config = ExperimentConfig()\n",
        "if vram_gb < config.min_vram_gb: raise ValueError('Insufficient VRAM')\n",
        "torch.manual_seed(config.seed)\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZATN8NdXdGI4"
      },
      "outputs": [],
      "source": [
        "### 2️⃣ Utility – Attention Implementation Fallback\n",
        "\n",
        "def get_fallback_chain(requested: str) -> List[str]:\n",
        "    # Always use eager attention for Phi-3 compatibility\n",
        "    return ['eager']\n",
        "\n",
        "def resolve_attn_impl(requested: str) -> str:\n",
        "    return get_fallback_chain(requested)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8wT0SLEUdGI5"
      },
      "outputs": [],
      "source": [
        "### 3️⃣ Utility – Robust Embedding Builder\n",
        "\n",
        "def build_inputs_embeds(model, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "    token_embeds = model.get_input_embeddings()(input_ids)\n",
        "    seq_len = input_ids.size(1)\n",
        "    position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "    pos_attrs = [\n",
        "        ('model.embed_positions', lambda m: getattr(m.model, 'embed_positions', None)),\n",
        "        ('wpe', lambda m: getattr(m.transformer, 'wpe', None) if hasattr(m, 'transformer') else None),\n",
        "        ('embeddings.position_embeddings', lambda m: getattr(m.embeddings, 'position_embeddings', None) if hasattr(m, 'embeddings') else None)\n",
        "    ]\n",
        "    for _, getter in pos_attrs:\n",
        "        pos_layer = getter(model)\n",
        "        if pos_layer is not None and callable(pos_layer):\n",
        "            pos_embeds = pos_layer(position_ids)\n",
        "            if pos_embeds is not None and pos_embeds.shape[:2] == token_embeds.shape[:2]:\n",
        "                token_embeds += pos_embeds\n",
        "                break\n",
        "    return token_embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1NiC5xljdGI6"
      },
      "outputs": [],
      "source": [
        "### 4️⃣ Class DoublePassPhi3\n",
        "\n",
        "class DoublePassPhi3(torch.nn.Module):\n",
        "    def __init__(self, model_path: str, config: ExperimentConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda')\n",
        "        attn_impl = resolve_attn_impl(config.attn_impl)\n",
        "\n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path, torch_dtype=torch.float16, device_map='auto',\n",
        "                trust_remote_code=True, attn_implementation=attn_impl\n",
        "            )\n",
        "            print(f'✅ Model loaded successfully with {attn_impl} attention')\n",
        "        except Exception as e:\n",
        "            error_msg = f'❌ Failed to load model with {attn_impl} attention: {str(e)}'\n",
        "            raise RuntimeError(error_msg)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model.eval()\n",
        "        self.num_layers = len(self.model.model.layers)\n",
        "\n",
        "    # NO CHANGE to get_residual_stream, it is correct.\n",
        "    def get_residual_stream(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                           past_key_values=None, use_cache: bool = True) -> Tuple[torch.Tensor, torch.Tensor, Optional[List]]:\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            residual_stream = outputs.hidden_states[-1]\n",
        "            new_past_key_values = outputs.past_key_values if use_cache else None\n",
        "            return logits, residual_stream, new_past_key_values\n",
        "\n",
        "    # BUG FIX 1: Added position_ids to the layer call.\n",
        "    def second_pass_forward(self, residual_stream: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                      num_layers: Optional[int] = None) -> torch.Tensor:\n",
        "      with torch.no_grad():\n",
        "        hidden_states = self.model.model.norm(residual_stream) if self.config.residual_variant == 'layernorm' and hasattr(self.model.model, 'norm') else residual_stream\n",
        "        layers_to_use = min(num_layers or self.num_layers, self.num_layers)\n",
        "\n",
        "        # Create position_ids based on the sequence length of the residual stream\n",
        "        seq_len = residual_stream.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)\n",
        "\n",
        "        # The model expects a 4D attention mask for the layers. We can create it from the 2D mask.\n",
        "        # This is often handled internally, but being explicit is safer in custom loops.\n",
        "        causal_mask = self.model._prepare_decoder_attention_mask(\n",
        "            attention_mask, (1, seq_len), hidden_states, 0\n",
        "        )\n",
        "\n",
        "        for i in range(layers_to_use):\n",
        "            outputs = self.model.model.layers[i](\n",
        "                hidden_states,\n",
        "                attention_mask=causal_mask,\n",
        "                position_ids=position_ids, # Pass the position IDs\n",
        "                use_cache=False\n",
        "            )\n",
        "            hidden_states = outputs[0]\n",
        "\n",
        "        hidden_states = self.model.model.norm(hidden_states) if hasattr(self.model.model, 'norm') else hidden_states\n",
        "        return self.model.lm_head(hidden_states)\n",
        "\n",
        "    # BUG FIX 2: Completely rewritten generate loop for correct state management.\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 100) -> Dict:\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True).to(self.device)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        generated_token_ids = input_ids\n",
        "        entropies1, entropies2 = [], []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # --- Pass 1: Standard autoregressive step to get the next token logits ---\n",
        "            # For the first pass, we can use the efficient KV cache.\n",
        "            # We pass the *entire* generated sequence so far. The model's use_cache\n",
        "            # logic will handle only processing the last token.\n",
        "            outputs = self.model(\n",
        "                input_ids=generated_token_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                use_cache=True, # Note: This will be overwritten if past_key_values is passed\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "\n",
        "            # The logits for the VERY LAST token are what we need to predict the next one.\n",
        "            logits1 = outputs.logits[:, -1, :]\n",
        "\n",
        "            probs1 = F.softmax(logits1, dim=-1)\n",
        "            entropy1 = -torch.sum(probs1 * torch.log(probs1 + self.config.entropy_eps), dim=-1).item()\n",
        "            entropies1.append(entropy1)\n",
        "\n",
        "            final_logits = logits1\n",
        "            entropies2.append(entropy1) # Default for baseline\n",
        "\n",
        "            # --- Pass 2: The \"Double Pass\" logic ---\n",
        "            # This is only performed if not in baseline mode.\n",
        "            if self.config.pass_type != 'baseline':\n",
        "                # To get the full residual stream, we MUST run a forward pass\n",
        "                # on the entire sequence WITHOUT the KV cache. This is inefficient\n",
        "                # but is the only way to correctly implement the experimental idea.\n",
        "                with torch.no_grad():\n",
        "                    full_outputs = self.model(\n",
        "                        input_ids=generated_token_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        output_hidden_states=True,\n",
        "                        use_cache=False # CRITICAL: No cache to get the full residual\n",
        "                    )\n",
        "\n",
        "                full_residual_stream = full_outputs.hidden_states[-1]\n",
        "\n",
        "                num_l = self.config.second_pass_layers if self.config.pass_type == 'double_partial' else None\n",
        "                logits2 = self.second_pass_forward(full_residual_stream, attention_mask, num_l)\n",
        "\n",
        "                final_logits = logits2[:, -1, :] # We only care about the last token's logits\n",
        "\n",
        "                probs2 = F.softmax(final_logits, dim=-1)\n",
        "                entropy2 = -torch.sum(probs2 * torch.log(probs2 + self.config.entropy_eps), dim=-1).item()\n",
        "                entropies2[-1] = entropy2 # Overwrite the last entropy value\n",
        "\n",
        "\n",
        "            # --- Sampling ---\n",
        "            probs = F.softmax(final_logits / self.config.temperature, dim=-1)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > self.config.top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            probs[indices_to_remove] = 0.0\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Append the new token for the next iteration\n",
        "            generated_token_ids = torch.cat([generated_token_ids, next_token], dim=-1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones((1,1), device=self.device)], dim=-1)\n",
        "\n",
        "\n",
        "        generated_text = self.tokenizer.decode(generated_token_ids[0], skip_special_tokens=True)\n",
        "        # We need to remove the original prompt from the output\n",
        "        if generated_text.startswith(prompt):\n",
        "            generated_text = generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'generated_text': generated_text.strip(),\n",
        "            'entropies1': entropies1,\n",
        "            'entropies2': entropies2,\n",
        "            'num_tokens': len(entropies1),\n",
        "            'time': time.time() - start_time\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "0f2610380be84937830be2989461ac98",
            "cdf1ca8b58d64ad9832f951784b556c5",
            "5ab33e308f794ad7be9d7a929ad82c95",
            "2f0ca6d3a98f4c3f9f136edb8d50e074",
            "6931c202840b41fdaebb28945cebb20b",
            "1679ecc5b61746398bdfde59a5afc693",
            "9d9d4672ed8844c5b13cf80067034be2",
            "c19cb2afe952494fad7f0735d54ad82b",
            "0f778984e039426eb96f7707b3c1637a",
            "bd8f4cfe25664d7eb6d6be7ddfc6acd8",
            "7ea2c3d250744ec985aad0d2a10de5c1"
          ]
        },
        "id": "QWPFy9zOdGI8",
        "outputId": "ddb8b6ff-c4d4-44ac-8793-fdbf1567a2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f2610380be84937830be2989461ac98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully with eager attention\n",
            "Smoke test passed: the size of the dataset should be divisible by in 1.05s\n"
          ]
        }
      ],
      "source": [
        "### 5️⃣ Quick Smoke Test\n",
        "\n",
        "try:\n",
        "    model = DoublePassPhi3(config.model_path, config)\n",
        "    result = model.generate('Sanity check:', max_new_tokens=10)\n",
        "    print(f'Smoke test passed: {result[\"generated_text\"]} in {result[\"time\"]:.2f}s')\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f'Smoke test failed: {str(e)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xtfXJmZPdGI8"
      },
      "outputs": [],
      "source": [
        "### 6️⃣ Simple Benchmark System\n",
        "\n",
        "# Simple Q&A dataset for testing\n",
        "SIMPLE_QA_DATASET = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
        "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
        "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"Shakespeare\"},\n",
        "    {\"question\": \"What is the square root of 16?\", \"answer\": \"4\"},\n",
        "    {\"question\": \"What year did World War II end?\", \"answer\": \"1945\"},\n",
        "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"},\n",
        "    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo\"},\n",
        "    {\"question\": \"How many days are in a leap year?\", \"answer\": \"366\"},\n",
        "    {\"question\": \"What is the speed of light?\", \"answer\": \"299,792,458\"},\n",
        "]\n",
        "\n",
        "def simple_benchmark(model: DoublePassPhi3, dataset: List[Dict] = None) -> Dict:\n",
        "    \"\"\"Run a simple Q&A benchmark.\"\"\"\n",
        "    if dataset is None:\n",
        "        dataset = SIMPLE_QA_DATASET\n",
        "\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "    results = []\n",
        "    total_time = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item[\"question\"]\n",
        "        expected = item[\"answer\"].lower()\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = model.generate(question, max_new_tokens=20)\n",
        "        inference_time = time.time() - start_time\n",
        "        total_time += inference_time\n",
        "\n",
        "        generated = result['generated_text'].lower().strip()\n",
        "\n",
        "        # Simple scoring - check if expected answer is in generated text\n",
        "        is_correct = expected in generated or any(word in generated for word in expected.split())\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected,\n",
        "            'generated': result['generated_text'],\n",
        "            'correct': is_correct,\n",
        "            'time': inference_time\n",
        "        })\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_time = total_time / total\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_time': avg_time,\n",
        "        'total_time': total_time,\n",
        "        'samples': total,\n",
        "        'results': results\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9TglC5GbdGI8"
      },
      "outputs": [],
      "source": [
        "### 7️⃣ Simple Benchmark Runner\n",
        "\n",
        "def run_simple_benchmark(model: DoublePassPhi3) -> Dict:\n",
        "    \"\"\"Run the simple Q&A benchmark.\"\"\"\n",
        "    try:\n",
        "        print(f\"Running simple benchmark with {model.config.pass_type} method...\")\n",
        "        results = simple_benchmark(model)\n",
        "        print(f\"✅ Completed: {results['accuracy']:.2f} accuracy, {results['avg_time']:.2f}s avg time\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f'❌ Error in benchmark: {str(e)}')\n",
        "        return {'accuracy': 0.0, 'samples': 0, 'avg_time': 0.0, 'total_time': 0.0}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simple_benchmark(DoublePassPhi3(config.model_path, config))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "76c6e2963a304671a7a24c50e2873223",
            "4e1cc5cea20d4d0b82783c130738e010",
            "92807af59c5d41f0ac17a029190a1896",
            "285faa59d1dd4584bbd18f5f932d68d6",
            "a0b96b4207a44d0c950b4579960db9b5",
            "49947eab2cde4330a0198eb2d8b32209",
            "2ca4c1dff5df493aba7f32054bdc0fb7",
            "85c59e5ce0de4666b3ac0089624a3bd6",
            "0ed34f27280346cca8dd1064f3b01499",
            "fb994974b9d84940a000d70b380d4dd7",
            "f5f8a1b1a65c46a689be04573ac93e27"
          ]
        },
        "collapsed": true,
        "id": "bzOmAuk5t1xl",
        "outputId": "c74b8a93-5f2d-4ff0-8554-97827294e102"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76c6e2963a304671a7a24c50e2873223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully with eager attention\n",
            "Running simple benchmark with baseline method...\n",
            "✅ Completed: 0.80 accuracy, 1.04s avg time\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8,\n",
              " 'avg_time': 1.0375317096710206,\n",
              " 'total_time': 10.375317096710205,\n",
              " 'samples': 10,\n",
              " 'results': [{'question': 'What is the capital of France?',\n",
              "   'expected': 'paris',\n",
              "   'generated': '# Answer\\nThe capital of France is Paris.',\n",
              "   'correct': True,\n",
              "   'time': 0.7548317909240723},\n",
              "  {'question': 'What is 2 + 2?',\n",
              "   'expected': '4',\n",
              "   'generated': \"I know this one, it's 4. Now, can you give me a harder question\",\n",
              "   'correct': True,\n",
              "   'time': 1.1072328090667725},\n",
              "  {'question': 'What is the largest planet in our solar system?',\n",
              "   'expected': 'jupiter',\n",
              "   'generated': 'Jupiter is the largest planet in our solar system. It is a gas giant and is known for',\n",
              "   'correct': True,\n",
              "   'time': 1.1078200340270996},\n",
              "  {'question': 'Who wrote Romeo and Juliet?',\n",
              "   'expected': 'shakespeare',\n",
              "   'generated': '# Answer\\nWilliam Shakespeare wrote \"Romeo and Juliet.\"',\n",
              "   'correct': True,\n",
              "   'time': 0.984142541885376},\n",
              "  {'question': 'What is the square root of 16?',\n",
              "   'expected': '4',\n",
              "   'generated': '# Answer\\nThe square root of 16 is 4.',\n",
              "   'correct': True,\n",
              "   'time': 0.9916331768035889},\n",
              "  {'question': 'What year did World War II end?',\n",
              "   'expected': '1945',\n",
              "   'generated': 'Answer: World War II ended in the year 1945. This was a',\n",
              "   'correct': True,\n",
              "   'time': 1.088244915008545},\n",
              "  {'question': 'What is the chemical symbol for gold?',\n",
              "   'expected': 'au',\n",
              "   'generated': 'Gold\\'s chemical symbol is Au, derived from the Latin word \"aurum.\"',\n",
              "   'correct': True,\n",
              "   'time': 1.085278034210205},\n",
              "  {'question': 'What is the capital of Japan?',\n",
              "   'expected': 'tokyo',\n",
              "   'generated': 'The capital of Japan is Tokyo. Tokyo is the political, economic, and cultural center of Japan.',\n",
              "   'correct': True,\n",
              "   'time': 1.087764024734497},\n",
              "  {'question': 'How many days are in a leap year?',\n",
              "   'expected': '366',\n",
              "   'generated': 'To determine the number of days in a leap year, we need to know the difference between a',\n",
              "   'correct': False,\n",
              "   'time': 1.090019702911377},\n",
              "  {'question': 'What is the speed of light?',\n",
              "   'expected': '299,792,458',\n",
              "   'generated': 'I need the speed of light in a vacuum. Please provide the value in meters',\n",
              "   'correct': False,\n",
              "   'time': 1.0783500671386719}]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NbkyKqgYdGI9"
      },
      "outputs": [],
      "source": [
        "### 8️⃣ Single Experiment Driver\n",
        "\n",
        "def run_experiment(config: ExperimentConfig) -> Dict:\n",
        "    \"\"\"Run a single experiment with the given configuration.\"\"\"\n",
        "    print(f\"\\n🚀 Starting experiment: {config.pass_type}\")\n",
        "\n",
        "    model = DoublePassPhi3(config.model_path, config)\n",
        "    results = run_simple_benchmark(model)\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    path = Path(f'/content/drive/MyDrive/runs/{timestamp}-{config.pass_type}.json')\n",
        "    path.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "    print(f'📊 Results: Accuracy {results[\"accuracy\"]:.3f}, Avg Time {results[\"avg_time\"]:.2f}s')\n",
        "    print(f'💾 Saved to: {path}')\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LJuGxrDtdGI9"
      },
      "outputs": [],
      "source": [
        "### 9️⃣ Grid Search Launcher\n",
        "\n",
        "\n",
        "# Programmatic experiment grid generation\n",
        "def generate_experiment_configs(custom_experiments: List[Dict] = None) -> List[ExperimentConfig]:\n",
        "    \"\"\"Generate list of experiment configurations. Optionally add custom experiments.\"\"\"\n",
        "    experiments = []\n",
        "\n",
        "    # Standard experiment set\n",
        "    experiments.append(ExperimentConfig(pass_type='baseline', residual_variant='raw'))\n",
        "    experiments.append(ExperimentConfig(pass_type='double_full', residual_variant='raw'))\n",
        "    experiments.append(ExperimentConfig(pass_type='double_full', residual_variant='layernorm'))\n",
        "\n",
        "    # Systematic partial pass experiments\n",
        "    for layers in [1, 2, 4, 8]:\n",
        "        experiments.append(ExperimentConfig(\n",
        "            pass_type='double_partial',\n",
        "            second_pass_layers=layers,\n",
        "            residual_variant='raw'\n",
        "        ))\n",
        "\n",
        "    # Add custom experiments if provided\n",
        "    if custom_experiments:\n",
        "        for custom in custom_experiments:\n",
        "            experiments.append(ExperimentConfig(**custom))\n",
        "\n",
        "    return experiments\n",
        "\n",
        "def run_all_experiments(custom_experiments: List[Dict] = None):\n",
        "    \"\"\"Run all experiments and compare results.\"\"\"\n",
        "    exps = generate_experiment_configs(custom_experiments)\n",
        "    print(f'🔬 Running {len(exps)} experiments, estimated time: {len(exps)*5:.0f} minutes')\n",
        "\n",
        "    all_results = []\n",
        "    for i, exp in enumerate(exps, 1):\n",
        "        print(f\"\\n📊 Experiment {i}/{len(exps)}\")\n",
        "        try:\n",
        "            res = run_experiment(exp)\n",
        "            variant_name = f\"{exp.pass_type}_{exp.residual_variant}\"\n",
        "            if exp.second_pass_layers:\n",
        "                variant_name += f\"_L{exp.second_pass_layers}\"\n",
        "            all_results.append((variant_name, res['accuracy']))\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed experiment {exp.pass_type}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Sort by accuracy\n",
        "    all_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print('\\n🏆 LEADERBOARD:')\n",
        "    print('=' * 40)\n",
        "    for rank, (name, acc) in enumerate(all_results, 1):\n",
        "        print(f'{rank}. {name}: {acc:.3f}')\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "10A4G5HXdGI9"
      },
      "outputs": [],
      "source": [
        "### 🔟 Interactive Playground & Quick Test\n",
        "\n",
        "def playground():\n",
        "    \"\"\"Interactive playground to compare baseline vs double pass.\"\"\"\n",
        "    print(\"🎮 Interactive Playground - Compare Baseline vs Double Pass\")\n",
        "    print(\"Type 'q' to quit, or enter any prompt to test both methods.\\n\")\n",
        "\n",
        "    while True:\n",
        "        prompt = input('📝 Enter prompt (or q to quit): ')\n",
        "        if prompt.lower() == 'q': break\n",
        "\n",
        "        print(f\"\\n🔍 Testing prompt: '{prompt}'\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Baseline experiment\n",
        "        print(\"🚀 Running baseline...\")\n",
        "        base_config = ExperimentConfig(pass_type='baseline')\n",
        "        base_model = DoublePassPhi3(base_config.model_path, base_config)\n",
        "        base_res = base_model.generate(prompt, max_new_tokens=30)\n",
        "        del base_model\n",
        "\n",
        "        # Double pass experiment\n",
        "        print(\"🚀 Running double pass...\")\n",
        "        double_config = ExperimentConfig(pass_type='double_full')\n",
        "        double_model = DoublePassPhi3(double_config.model_path, double_config)\n",
        "        double_res = double_model.generate(prompt, max_new_tokens=30)\n",
        "        del double_model\n",
        "\n",
        "        # Clear CUDA cache to prevent memory leak\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate entropy differences\n",
        "        base_ent = sum(base_res['entropies1']) / len(base_res['entropies1']) if base_res['entropies1'] else 0\n",
        "        double_ent = sum(double_res['entropies2']) / len(double_res['entropies2']) if double_res['entropies2'] else 0\n",
        "\n",
        "        print(f\"\\n📊 RESULTS:\")\n",
        "        print(f\"🔵 Baseline: {base_res['generated_text']}\")\n",
        "        print(f\"   Time: {base_res['time']:.2f}s, Entropy: {base_ent:.3f}\")\n",
        "        print(f\"🟣 Double Pass: {double_res['generated_text']}\")\n",
        "        print(f\"   Time: {double_res['time']:.2f}s, Entropy: {double_ent:.3f}\")\n",
        "        print(f\"⚡ Speed ratio: {double_res['time']/base_res['time']:.1f}x slower\")\n",
        "        print(f\"🧠 Entropy change: {base_ent - double_ent:.3f}\\n\")\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Run a quick test of both methods.\"\"\"\n",
        "    print(\"🧪 Quick Test - Baseline vs Double Pass\\n\")\n",
        "\n",
        "    test_prompts = [\n",
        "        \"The capital of France is\",\n",
        "        \"2 + 2 equals\",\n",
        "        \"The largest planet is\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"🔍 Testing: '{prompt}'\")\n",
        "\n",
        "        # Baseline\n",
        "        base_config = ExperimentConfig(pass_type='baseline')\n",
        "        base_model = DoublePassPhi3(base_config.model_path, base_config)\n",
        "        base_res = base_model.generate(prompt, max_new_tokens=10)\n",
        "        del base_model\n",
        "\n",
        "        # Double pass\n",
        "        double_config = ExperimentConfig(pass_type='double_full')\n",
        "        double_model = DoublePassPhi3(double_config.model_path, double_config)\n",
        "        double_res = double_model.generate(prompt, max_new_tokens=10)\n",
        "        del double_model\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"  🔵 Baseline: {base_res['generated_text']}\")\n",
        "        print(f\"  🟣 Double: {double_res['generated_text']}\")\n",
        "        print(f\"  ⚡ Speed: {double_res['time']/base_res['time']:.1f}x slower\\n\")\n",
        "\n",
        "# Uncomment to run:\n",
        "# playground()\n",
        "# quick_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: GSM8K BENCHMARKING SYSTEM\n",
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "def parse_gsm8k_answer(generated_text: str) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Parses the model's generated text to find the final numerical answer for GSM8K.\n",
        "    It looks for the '####' marker or the last number in the string.\n",
        "    \"\"\"\n",
        "    # Look for the #### pattern, which is standard for GSM8K reasoning chains\n",
        "    match = re.search(r\"####\\s*([0-9,.]+)\", generated_text)\n",
        "    if match:\n",
        "        # Extract number, remove commas, and convert to float\n",
        "        return float(match.group(1).replace(',', ''))\n",
        "\n",
        "    # If #### is not found, fall back to finding the last number in the string\n",
        "    # This is less reliable but a good fallback.\n",
        "    matches = re.findall(r\"([0-9,.]+)\", generated_text)\n",
        "    if matches:\n",
        "        return float(matches[-1].replace(',', ''))\n",
        "\n",
        "    return None # Return None if no number is found\n",
        "\n",
        "def run_gsm8k_benchmark(model: DoublePassPhi3, num_questions: int = 100) -> Dict:\n",
        "    \"\"\"\n",
        "    Runs a benchmark on the GSM8K dataset.\n",
        "\n",
        "    Args:\n",
        "        model: An initialized DoublePassPhi3 model.\n",
        "        num_questions: The number of questions to run from the test set.\n",
        "                       Set to 'all' to run the entire benchmark (takes a long time!).\n",
        "    \"\"\"\n",
        "    print(f\"🚀 Loading GSM8K dataset...\")\n",
        "    try:\n",
        "        # Load the 'main' configuration of gsm8k\n",
        "        dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load dataset: {e}\")\n",
        "        return {}\n",
        "\n",
        "    if num_questions != 'all':\n",
        "        dataset = dataset.select(range(num_questions))\n",
        "        print(f\"📊 Running on the first {num_questions} questions of the GSM8K test set.\")\n",
        "    else:\n",
        "        print(f\"📊 Running on the ENTIRE {len(dataset)} questions of the GSM8K test set. This will take a while!\")\n",
        "\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "    results = []\n",
        "    total_time = 0\n",
        "\n",
        "    for i, item in enumerate(dataset):\n",
        "        question = item[\"question\"]\n",
        "        # The expected answer is just the number\n",
        "        expected_answer_text = item[\"answer\"].split(\"####\")[-1].strip()\n",
        "        expected = float(expected_answer_text.replace(',', ''))\n",
        "\n",
        "        # Generate an answer. We need more tokens for GSM8K's reasoning.\n",
        "        result = model.generate(question, max_new_tokens=256)\n",
        "        total_time += result['time']\n",
        "\n",
        "        # Parse the final number from the model's output\n",
        "        generated = parse_gsm8k_answer(result['generated_text'])\n",
        "        is_correct = generated is not None and generated == expected\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        print(f\"  Q{i+1}/{total}: Expected: {expected}, Got: {generated} -> {'✅ Correct' if is_correct else '❌ Incorrect'}\")\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected,\n",
        "            'generated_text': result['generated_text'],\n",
        "            'parsed_answer': generated,\n",
        "            'correct': is_correct,\n",
        "            'time': result['time']\n",
        "        })\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    avg_time = total_time / total if total > 0 else 0\n",
        "\n",
        "    benchmark_results = {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_time': avg_time,\n",
        "        'total_time': total_time,\n",
        "        'samples': total,\n",
        "        'pass_type': model.config.pass_type,\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    print(f\"\\n✅ GSM8K BENCHMARK COMPLETE ({model.config.pass_type})\")\n",
        "    print(f\"   Accuracy: {accuracy:.3f} ({correct}/{total})\")\n",
        "    print(f\"   Average Time per Question: {avg_time:.2f}s\")\n",
        "\n",
        "    return benchmark_results"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T30HFuZ7tD3l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to run the GSM8K benchmark comparison\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set the number of questions you want to test.\n",
        "# Use a small number like 20 for a quick test.\n",
        "# Use 'all' for the full benchmark (can take hours).\n",
        "NUM_QUESTIONS_TO_RUN = 20\n",
        "\n",
        "# --- 1. RUN BASELINE MODEL ---\n",
        "print(\"=\"*60)\n",
        "print(\"📊 STARTING GSM8K BENCHMARK: BASELINE\")\n",
        "print(\"=\"*60)\n",
        "# Create a config for the baseline run\n",
        "baseline_config = ExperimentConfig(pass_type='baseline')\n",
        "# Initialize the model with this config\n",
        "baseline_model = DoublePassPhi3(baseline_config.model_path, baseline_config)\n",
        "# Run the benchmark\n",
        "baseline_results = run_gsm8k_benchmark(baseline_model, num_questions=NUM_QUESTIONS_TO_RUN)\n",
        "# Clean up memory\n",
        "del baseline_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 2. RUN DOUBLE PASS MODEL ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 STARTING GSM8K BENCHMARK: DOUBLE PASS (FULL)\")\n",
        "print(\"=\"*60)\n",
        "# Create a config for your experimental run\n",
        "# We will test a full double pass here\n",
        "double_pass_config = ExperimentConfig(\n",
        "    pass_type='double_full',\n",
        "    residual_variant='raw' # or 'layernorm'\n",
        ")\n",
        "# Initialize the model\n",
        "double_pass_model = DoublePassPhi3(double_pass_config.model_path, double_pass_config)\n",
        "# Run the benchmark\n",
        "double_pass_results = run_gsm8k_benchmark(double_pass_model, num_questions=NUM_QUESTIONS_TO_RUN)\n",
        "# Clean up memory\n",
        "del double_pass_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 3. COMPARE RESULTS ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 FINAL GSM8K BENCHMARK COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "if baseline_results:\n",
        "    print(f\"🔵 Baseline Accuracy: {baseline_results['accuracy']:.3f}\")\n",
        "    print(f\"   Avg. Time: {baseline_results['avg_time']:.2f}s\")\n",
        "if double_pass_results:\n",
        "    print(f\"🟣 Double Pass Accuracy: {double_pass_results['accuracy']:.3f}\")\n",
        "    print(f\"   Avg. Time: {double_pass_results['avg_time']:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766,
          "referenced_widgets": [
            "75e3ff2123c548ec8c9a49481aa25343",
            "af349aab3cc54251b6d5e1562ec8b24f",
            "495187c32cb34e68b7c120602381d73c",
            "cce6d9f3cefe4f64b246585f00692f48",
            "a24abcd1052549ffafda47a4e17fdfde",
            "e7cc2c310db04756b8b1c5b3d3a649fd",
            "30fdc2c4bfe74b1193ff4c3bcea10f7a",
            "a8a2bfaef7ae423292c8c87b22cf890f",
            "738264b16069419385a98f71dcc26375",
            "d7b96aa2b3604d57a9521985f0389ea4",
            "9d17e7783cc447b8aacf4d44fe86d848",
            "cf3932b2b03c4ac58367ae23a57bf605",
            "723ec0032e334d89897965258807f293",
            "617f3f54e143426cad4b5e01557edc0d",
            "0492abe8a01e428cbd5e49e98971c9ea",
            "882a0ca4d79c40eab02ce5e82659c6c7",
            "5e91f3cc95034bd08258766711a9f329",
            "07d35989c9da401883a158632a4dbedb",
            "3b3ccbf08ffe4060a77fc4bdbbb9f0c9",
            "0c357708966e4dfbbdec20efcfec58c2",
            "9500b4284c6640649a09d570b262ac9b",
            "ab4b9a36e61a415998613e2fc2117564",
            "b9dcf2ef007d4c93a3a99d38e47fbc00",
            "673dcc34de9f4a72a7107123985b81ec",
            "aca2f33e658a42c4bb9decd91eb5eea8",
            "6b69c3a6941b43e3a3732efb7721a5b3",
            "492c14e6007443d3b9357551961fbb69",
            "b3466038db654a4585f78be1519a395b",
            "58499ff61ac34a79a0c1a3ae5f4ccf0c",
            "afa499b9a85a4f31a4ab41b19197b154",
            "3bac6f30901741039f28367335cbe992",
            "5da1fd49802f471faf3f8ae7798c0913",
            "029f18254fbc4145964f3f30a74a640d"
          ]
        },
        "id": "nOfjvEXO5Eh7",
        "outputId": "fcc70574-f197-4822-f28d-489c09f5af81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📊 STARTING GSM8K BENCHMARK: BASELINE\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75e3ff2123c548ec8c9a49481aa25343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully with eager attention\n",
            "🚀 Loading GSM8K dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Downloading data: 100%|██████████| 2.31M/2.31M [00:00<00:00, 9.12MB/s]\n",
            "Downloading data: 100%|██████████| 419k/419k [00:00<00:00, 3.79MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf3932b2b03c4ac58367ae23a57bf605"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9dcf2ef007d4c93a3a99d38e47fbc00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Running on the first 20 questions of the GSM8K test set.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '.'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-3511375206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoublePassPhi3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Run the benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbaseline_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_gsm8k_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_questions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_QUESTIONS_TO_RUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# Clean up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-2297657272.py\u001b[0m in \u001b[0;36mrun_gsm8k_benchmark\u001b[0;34m(model, num_questions)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Parse the final number from the model's output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_gsm8k_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mis_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-2297657272.py\u001b[0m in \u001b[0;36mparse_gsm8k_answer\u001b[0;34m(generated_text)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"([0-9,.]+)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# Return None if no number is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxo68mIudGI9"
      },
      "outputs": [],
      "source": [
        "### 1️⃣1️⃣ Results Analysis & Plots\n",
        "\n",
        "def analyze_results(runs_dir='/content/drive/MyDrive/runs'):\n",
        "    \"\"\"Analyze experiment results with visualization.\"\"\"\n",
        "    files = glob(f'{runs_dir}/*.json')\n",
        "    if not files:\n",
        "        print('No results found');\n",
        "        return\n",
        "\n",
        "    data = []\n",
        "    for f in files:\n",
        "        with open(f) as jf:\n",
        "            res = json.load(jf)\n",
        "            variant = Path(f).stem.split('-')[1]\n",
        "            for bench, metrics in res.items():\n",
        "                data.append({\n",
        "                    'variant': variant,\n",
        "                    'benchmark': bench,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'runtime': metrics['runtime']\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "    # Average accuracy by variant\n",
        "    df.groupby('variant')['accuracy'].mean().sort_values(ascending=False).plot.bar(ax=axs[0])\n",
        "    axs[0].set_title('Avg Accuracy by Variant')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "\n",
        "    # Accuracy vs Runtime scatter\n",
        "    df.plot.scatter(x='runtime', y='accuracy', ax=axs[1])\n",
        "    axs[1].set_title('Accuracy vs Runtime')\n",
        "    axs[1].set_xlabel('Runtime (seconds)')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{runs_dir}/analysis.png')\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "# analyze_results()  # Uncomment to run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9J9O0pTdGI9"
      },
      "outputs": [],
      "source": [
        "### 1️⃣2️⃣ Usage Examples & How to Run\n",
        "\n",
        "print(\"✅ All functions loaded successfully!\")\n",
        "print(\"\\n🚀 HOW TO RUN THE EXPERIMENT:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n1️⃣ QUICK START:\")\n",
        "print(\"   quick_test()  # Run quick comparison\")\n",
        "\n",
        "print(\"\\n2️⃣ SINGLE EXPERIMENTS:\")\n",
        "print(\"   # Baseline method\")\n",
        "print(\"   baseline_config = ExperimentConfig(pass_type='baseline')\")\n",
        "print(\"   result1 = run_experiment(baseline_config)\")\n",
        "print(\"\")\n",
        "print(\"   # Double pass method\")\n",
        "print(\"   double_config = ExperimentConfig(pass_type='double_full')\")\n",
        "print(\"   result2 = run_experiment(double_config)\")\n",
        "\n",
        "print(\"\\n3️⃣ FULL COMPARISON:\")\n",
        "print(\"   run_all_experiments()  # Compare all methods\")\n",
        "\n",
        "print(\"\\n4️⃣ INTERACTIVE MODE:\")\n",
        "print(\"   playground()  # Test custom prompts\")\n",
        "\n",
        "print(\"\\n5️⃣ CUSTOM EXPERIMENTS:\")\n",
        "print(\"   custom_exps = [\")\n",
        "print(\"       {'pass_type': 'double_partial', 'second_pass_layers': 4}\")\n",
        "print(\"   ]\")\n",
        "print(\"   run_all_experiments(custom_experiments=custom_exps)\")\n",
        "\n",
        "print(\"\\n💡 TIPS:\")\n",
        "print(\"- Run smoke test first to verify model loading\")\n",
        "print(\"- Start with quick_test() for immediate results\")\n",
        "print(\"- Each experiment takes ~2-5 minutes\")\n",
        "print(\"- Results saved to /content/drive/MyDrive/runs/\")\n",
        "print(\"- Use Ctrl+C to stop long-running experiments\")\n",
        "\n",
        "print(\"\\n🎯 RECOMMENDED WORKFLOW:\")\n",
        "print(\"1. Run the smoke test (cell 5)\")\n",
        "print(\"2. Try quick_test() for immediate comparison\")\n",
        "print(\"3. Run single experiments to understand differences\")\n",
        "print(\"4. Use playground() to test specific prompts\")\n",
        "print(\"5. Run full comparison with run_all_experiments()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJwt6aQpdGI9"
      },
      "outputs": [],
      "source": [
        "# Additional utility functions and debug helpers\n",
        "\n",
        "def print_model_summary(model):\n",
        "    \"\"\"Print model architecture summary.\"\"\"\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(f\"Device: {model.device}\")\n",
        "    print(f\"Layers: {model.num_layers}\")\n",
        "    print(f\"Config: {model.config}\")\n",
        "\n",
        "# Quick profiling: torch.utils.bottleneck.run(model.generate('test'))\n",
        "# Toggle attention: config.attn_impl = 'eager'\n",
        "# Memory check: print(f\"VRAM used: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_rDStfJdGI-"
      },
      "outputs": [],
      "source": [
        "### 1️⃣4️⃣ Cleanup & Tips\n",
        "\n",
        "\n",
        "# Tips:\n",
        "# - Restart runtime if OOM: Runtime > Restart session\n",
        "# - Download results: Files > Mount Drive > Copy from /content/drive/MyDrive/runs\n",
        "# - Expected times: Baseline ~30min/benchmark, Double ~60min\n",
        "# - Checklist: Run smoke test, check GPU, verify model path, start with single exp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGjnf9gvdGI-"
      },
      "outputs": [],
      "source": [
        "### 1️⃣5️⃣ Ready to Run! 🚀\n",
        "\n",
        "print(\"🎉 SETUP COMPLETE!\")\n",
        "print(\"All functions are loaded and ready to use.\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 TO START EXPERIMENTING, RUN ONE OF THESE:\")\n",
        "print(\"=\"*60)\n",
        "print(\"👉 quick_test()           # 2-minute quick demo\")\n",
        "print(\"👉 playground()           # Interactive testing\")\n",
        "print(\"👉 run_all_experiments()  # Full comparison (~30 min)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n💡 WHAT EACH METHOD DOES:\")\n",
        "print(\"🔵 Baseline: Normal single forward pass (standard AI)\")\n",
        "print(\"🟣 Double Pass: Runs model twice for each word (experimental)\")\n",
        "print(\"\\n🎯 GOAL: See if 'thinking twice' improves accuracy!\")\n",
        "\n",
        "# Uncomment any of these to run:\n",
        "# quick_test()\n",
        "# playground()\n",
        "# run_all_experiments()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f2610380be84937830be2989461ac98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf1ca8b58d64ad9832f951784b556c5",
              "IPY_MODEL_5ab33e308f794ad7be9d7a929ad82c95",
              "IPY_MODEL_2f0ca6d3a98f4c3f9f136edb8d50e074"
            ],
            "layout": "IPY_MODEL_6931c202840b41fdaebb28945cebb20b"
          }
        },
        "cdf1ca8b58d64ad9832f951784b556c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1679ecc5b61746398bdfde59a5afc693",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9d4672ed8844c5b13cf80067034be2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5ab33e308f794ad7be9d7a929ad82c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19cb2afe952494fad7f0735d54ad82b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f778984e039426eb96f7707b3c1637a",
            "value": 2
          }
        },
        "2f0ca6d3a98f4c3f9f136edb8d50e074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8f4cfe25664d7eb6d6be7ddfc6acd8",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea2c3d250744ec985aad0d2a10de5c1",
            "value": " 2/2 [00:06&lt;00:00,  2.92s/it]"
          }
        },
        "6931c202840b41fdaebb28945cebb20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1679ecc5b61746398bdfde59a5afc693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9d4672ed8844c5b13cf80067034be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c19cb2afe952494fad7f0735d54ad82b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f778984e039426eb96f7707b3c1637a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd8f4cfe25664d7eb6d6be7ddfc6acd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea2c3d250744ec985aad0d2a10de5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c6e2963a304671a7a24c50e2873223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e1cc5cea20d4d0b82783c130738e010",
              "IPY_MODEL_92807af59c5d41f0ac17a029190a1896",
              "IPY_MODEL_285faa59d1dd4584bbd18f5f932d68d6"
            ],
            "layout": "IPY_MODEL_a0b96b4207a44d0c950b4579960db9b5"
          }
        },
        "4e1cc5cea20d4d0b82783c130738e010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49947eab2cde4330a0198eb2d8b32209",
            "placeholder": "​",
            "style": "IPY_MODEL_2ca4c1dff5df493aba7f32054bdc0fb7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "92807af59c5d41f0ac17a029190a1896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c59e5ce0de4666b3ac0089624a3bd6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ed34f27280346cca8dd1064f3b01499",
            "value": 2
          }
        },
        "285faa59d1dd4584bbd18f5f932d68d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb994974b9d84940a000d70b380d4dd7",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f8a1b1a65c46a689be04573ac93e27",
            "value": " 2/2 [00:06&lt;00:00,  3.03s/it]"
          }
        },
        "a0b96b4207a44d0c950b4579960db9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49947eab2cde4330a0198eb2d8b32209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca4c1dff5df493aba7f32054bdc0fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85c59e5ce0de4666b3ac0089624a3bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed34f27280346cca8dd1064f3b01499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb994974b9d84940a000d70b380d4dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f8a1b1a65c46a689be04573ac93e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75e3ff2123c548ec8c9a49481aa25343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af349aab3cc54251b6d5e1562ec8b24f",
              "IPY_MODEL_495187c32cb34e68b7c120602381d73c",
              "IPY_MODEL_cce6d9f3cefe4f64b246585f00692f48"
            ],
            "layout": "IPY_MODEL_a24abcd1052549ffafda47a4e17fdfde"
          }
        },
        "af349aab3cc54251b6d5e1562ec8b24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7cc2c310db04756b8b1c5b3d3a649fd",
            "placeholder": "​",
            "style": "IPY_MODEL_30fdc2c4bfe74b1193ff4c3bcea10f7a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "495187c32cb34e68b7c120602381d73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8a2bfaef7ae423292c8c87b22cf890f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_738264b16069419385a98f71dcc26375",
            "value": 2
          }
        },
        "cce6d9f3cefe4f64b246585f00692f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b96aa2b3604d57a9521985f0389ea4",
            "placeholder": "​",
            "style": "IPY_MODEL_9d17e7783cc447b8aacf4d44fe86d848",
            "value": " 2/2 [00:06&lt;00:00,  3.02s/it]"
          }
        },
        "a24abcd1052549ffafda47a4e17fdfde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7cc2c310db04756b8b1c5b3d3a649fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30fdc2c4bfe74b1193ff4c3bcea10f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8a2bfaef7ae423292c8c87b22cf890f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738264b16069419385a98f71dcc26375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7b96aa2b3604d57a9521985f0389ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d17e7783cc447b8aacf4d44fe86d848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf3932b2b03c4ac58367ae23a57bf605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_723ec0032e334d89897965258807f293",
              "IPY_MODEL_617f3f54e143426cad4b5e01557edc0d",
              "IPY_MODEL_0492abe8a01e428cbd5e49e98971c9ea"
            ],
            "layout": "IPY_MODEL_882a0ca4d79c40eab02ce5e82659c6c7"
          }
        },
        "723ec0032e334d89897965258807f293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e91f3cc95034bd08258766711a9f329",
            "placeholder": "​",
            "style": "IPY_MODEL_07d35989c9da401883a158632a4dbedb",
            "value": "Generating train split: 100%"
          }
        },
        "617f3f54e143426cad4b5e01557edc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b3ccbf08ffe4060a77fc4bdbbb9f0c9",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c357708966e4dfbbdec20efcfec58c2",
            "value": 7473
          }
        },
        "0492abe8a01e428cbd5e49e98971c9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9500b4284c6640649a09d570b262ac9b",
            "placeholder": "​",
            "style": "IPY_MODEL_ab4b9a36e61a415998613e2fc2117564",
            "value": " 7473/7473 [00:00&lt;00:00, 117721.86 examples/s]"
          }
        },
        "882a0ca4d79c40eab02ce5e82659c6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e91f3cc95034bd08258766711a9f329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07d35989c9da401883a158632a4dbedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b3ccbf08ffe4060a77fc4bdbbb9f0c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c357708966e4dfbbdec20efcfec58c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9500b4284c6640649a09d570b262ac9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab4b9a36e61a415998613e2fc2117564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9dcf2ef007d4c93a3a99d38e47fbc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673dcc34de9f4a72a7107123985b81ec",
              "IPY_MODEL_aca2f33e658a42c4bb9decd91eb5eea8",
              "IPY_MODEL_6b69c3a6941b43e3a3732efb7721a5b3"
            ],
            "layout": "IPY_MODEL_492c14e6007443d3b9357551961fbb69"
          }
        },
        "673dcc34de9f4a72a7107123985b81ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3466038db654a4585f78be1519a395b",
            "placeholder": "​",
            "style": "IPY_MODEL_58499ff61ac34a79a0c1a3ae5f4ccf0c",
            "value": "Generating test split: 100%"
          }
        },
        "aca2f33e658a42c4bb9decd91eb5eea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa499b9a85a4f31a4ab41b19197b154",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bac6f30901741039f28367335cbe992",
            "value": 1319
          }
        },
        "6b69c3a6941b43e3a3732efb7721a5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da1fd49802f471faf3f8ae7798c0913",
            "placeholder": "​",
            "style": "IPY_MODEL_029f18254fbc4145964f3f30a74a640d",
            "value": " 1319/1319 [00:00&lt;00:00, 104974.99 examples/s]"
          }
        },
        "492c14e6007443d3b9357551961fbb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3466038db654a4585f78be1519a395b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58499ff61ac34a79a0c1a3ae5f4ccf0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afa499b9a85a4f31a4ab41b19197b154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bac6f30901741039f28367335cbe992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5da1fd49802f471faf3f8ae7798c0913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "029f18254fbc4145964f3f30a74a640d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}