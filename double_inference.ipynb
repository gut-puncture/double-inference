{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/double-inference/blob/main/double_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OxGBjdi75Q9e",
      "metadata": {
        "id": "OxGBjdi75Q9e",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Double Forward Pass Research: Phi-3 Mini 4K Instruct\n",
        "\n",
        "## Research Objective\n",
        "Quantitatively test whether performing TWO sequential forward passes per generated token (feeding the final residual stream of pass #1 back into layer #0) improves language-reasoning ability of the 3.8B-parameter \"microsoft/phi-3-mini-4k-instruct\" model.\n",
        "\n",
        "## Hypothesis\n",
        "Extra compute refines internal representations, lowering token-level entropy and boosting accuracy on reasoning benchmarks relative to the single-pass baseline.\n",
        "\n",
        "## Benchmarks\n",
        "- **MMLU**: 57 subjects, 5-shot, accuracy\n",
        "- **BigBench-Hard (BBH)**: 23 tasks, 0-shot, accuracy\n",
        "- **GSM8K**: Math word problems, 5-shot chain-of-thought, answer accuracy\n",
        "\n",
        "## Experimental Conditions\n",
        "1. Baseline (single-pass generation)\n",
        "2. Double-pass (full N layers)\n",
        "3. Partial-pass grid (k ∈ {1, 2, 4, 8, N/2, N})\n",
        "4. Residual-norm ablation (raw vs LayerNorm)\n",
        "\n",
        "## Environment Setup\n",
        "- Google Colab A100 GPU\n",
        "- Model path: `/content/drive/MyDrive/phi3_3.8B`\n",
        "- All logs saved locally (no external services)\n",
        "\n",
        "## Expected Runtime\n",
        "~10 GPU-hours total for all variants across three benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "32d11589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32d11589",
        "outputId": "834a700d-f964-4205-8d8c-118337638dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n",
            "✓ Model found at /content/drive/MyDrive/phi3_3.8B\n",
            "Contents: ['.cache', 'LICENSE', 'added_tokens.json', '.gitattributes', 'README.md', 'SECURITY.md', 'NOTICE.md', 'config.json', 'CODE_OF_CONDUCT.md', 'configuration_phi3.py', 'generation_config.json', 'model.safetensors.index.json', 'modeling_phi3.py', 'special_tokens_map.json', 'sample_finetune.py', 'tokenizer_config.json', 'tokenizer.model', 'tokenizer.json', 'model-00002-of-00002.safetensors', 'model-00001-of-00002.safetensors']\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers>=4.41.0 accelerate torch tensorboard lm-eval[api] datasets\n",
        "# Skip flash-attn installation - we'll use auto-fallback to SDPA instead\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify model path exists\n",
        "import os\n",
        "model_path = \"/content/drive/MyDrive/phi3_3.8B\"\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"✓ Model found at {model_path}\")\n",
        "    print(f\"Contents: {os.listdir(model_path)}\")\n",
        "else:\n",
        "    print(f\"✗ Model not found at {model_path}\")\n",
        "    print(\"Please ensure the model is uploaded to the correct location\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78173e5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78173e5f",
        "outputId": "3fe11ae9-be81-47ff-8a7c-f5f6f71c3684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.5 GB\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
        "    GenerationConfig, set_seed\n",
        ")\n",
        "from transformers.models.phi3.modeling_phi3 import Phi3Model, Phi3ForCausalLM\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional, Tuple, Any, Union\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f6a02b0",
      "metadata": {
        "id": "1f6a02b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892b1b5b-a2b9-414e-a7c2-43887519841b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration and logging utilities loaded\n"
          ]
        }
      ],
      "source": [
        "# Configuration and logging utilities\n",
        "\n",
        "def resolve_attn_impl(requested: str) -> str:\n",
        "    \"\"\"\n",
        "    Resolve attention implementation with automatic fallback.\n",
        "\n",
        "    Args:\n",
        "        requested: 'auto', 'flash2', 'sdpa', or 'eager'\n",
        "\n",
        "    Returns:\n",
        "        The actual implementation string for transformers\n",
        "    \"\"\"\n",
        "    if requested in {\"sdpa\", \"eager\"}:\n",
        "        return requested\n",
        "    if requested == \"flash2\":\n",
        "        return \"flash_attention_2\"\n",
        "\n",
        "    # Auto-detect: try flash attention first, fallback to sdpa\n",
        "    try:\n",
        "        import flash_attn\n",
        "        # Test if flash_attn can actually be imported without errors\n",
        "        from flash_attn import flash_attn_func\n",
        "        logger.info(\"✓ Flash Attention 2 available - using flash_attention_2\")\n",
        "        return \"flash_attention_2\"\n",
        "    except (ImportError, Exception) as e:\n",
        "        logger.info(f\"Flash Attention 2 not available ({str(e)[:50]}...) - falling back to SDPA\")\n",
        "        return \"sdpa\"\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Configuration for double-pass experiments\"\"\"\n",
        "    model_path: str\n",
        "    pass_type: str  # 'baseline', 'double_full', 'double_partial'\n",
        "    second_pass_layers: Optional[int] = None\n",
        "    residual_variant: str = 'raw'  # 'raw' or 'layernorm'\n",
        "    attn_impl: str = 'auto'  # 'auto', 'flash2', 'sdpa', 'eager'\n",
        "    seed: int = 42\n",
        "    max_length: int = 2048\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "    do_sample: bool = True\n",
        "    timestamp: str = None\n",
        "    git_sha: str = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.timestamp is None:\n",
        "            self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        if self.git_sha is None:\n",
        "            try:\n",
        "                self.git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
        "            except:\n",
        "                self.git_sha = \"unknown\"\n",
        "\n",
        "class ExpLogger:\n",
        "    \"\"\"Local experiment logger - no external services\"\"\"\n",
        "\n",
        "    def __init__(self, save_dir: str):\n",
        "        self.save_dir = Path(save_dir)\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize files\n",
        "        self.token_stats_file = self.save_dir / \"token_stats.jsonl\"\n",
        "        self.results_file = self.save_dir / \"results.csv\"\n",
        "        self.tb_dir = self.save_dir / \"tb\"\n",
        "        self.tb_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # TensorBoard writer\n",
        "        self.tb_writer = SummaryWriter(str(self.tb_dir))\n",
        "\n",
        "        # Initialize CSV\n",
        "        self.csv_headers = [\n",
        "            'benchmark', 'variant', 'accuracy', 'num_samples',\n",
        "            'avg_entropy_base', 'avg_entropy_double', 'runtime_sec',\n",
        "            'tokens_generated', 'compute_overhead'\n",
        "        ]\n",
        "\n",
        "        with open(self.results_file, 'w', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
        "            writer.writeheader()\n",
        "\n",
        "    def log_token_stats(self, stats: Dict[str, Any]):\n",
        "        \"\"\"Log per-token statistics\"\"\"\n",
        "        with open(self.token_stats_file, 'a') as f:\n",
        "            f.write(json.dumps(stats) + '\\n')\n",
        "\n",
        "    def log_scalar(self, tag: str, value: float, step: int):\n",
        "        \"\"\"Log scalar to TensorBoard\"\"\"\n",
        "        self.tb_writer.add_scalar(tag, value, step)\n",
        "\n",
        "    def log_histogram(self, tag: str, values: np.ndarray, step: int):\n",
        "        \"\"\"Log histogram to TensorBoard\"\"\"\n",
        "        self.tb_writer.add_histogram(tag, values, step)\n",
        "\n",
        "    def log_text(self, tag: str, text: str, step: int):\n",
        "        \"\"\"Log text to TensorBoard\"\"\"\n",
        "        self.tb_writer.add_text(tag, text, step)\n",
        "\n",
        "    def log_results(self, results: Dict[str, Any]):\n",
        "        \"\"\"Log aggregate results to CSV\"\"\"\n",
        "        with open(self.results_file, 'a', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
        "            writer.writerow(results)\n",
        "\n",
        "    def save_config(self, config: ExperimentConfig):\n",
        "        \"\"\"Save experiment configuration\"\"\"\n",
        "        config_path = self.save_dir / \"run_config.json\"\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(asdict(config), f, indent=2)\n",
        "\n",
        "    def finalize(self):\n",
        "        \"\"\"Close TensorBoard writer\"\"\"\n",
        "        self.tb_writer.close()\n",
        "\n",
        "        # Create summary\n",
        "        summary_path = self.save_dir / \"summary.txt\"\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(f\"Experiment completed at {datetime.now()}\\n\")\n",
        "            f.write(f\"Results saved to: {self.save_dir}\\n\")\n",
        "            f.write(f\"View TensorBoard: tensorboard --logdir {self.tb_dir}\\n\")\n",
        "\n",
        "print(\"✓ Configuration and logging utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a5c09008",
      "metadata": {
        "id": "a5c09008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b55d03-2261-42d7-91c6-4310d18346e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ DoublePassPhi3 model implementation loaded\n"
          ]
        }
      ],
      "source": [
        "# Double-pass Phi3 model implementation\n",
        "\n",
        "class DoublePassPhi3(nn.Module):\n",
        "    \"\"\"\n",
        "    Phi3 model wrapper that supports double forward passes.\n",
        "\n",
        "    The key insight: after the first forward pass, we take the residual stream\n",
        "    (hidden states before final LayerNorm) and feed it back into the transformer\n",
        "    blocks for a second pass, then sample from the second pass logits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, config: ExperimentConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Resolve attention implementation\n",
        "        attn_impl = resolve_attn_impl(config.attn_impl)\n",
        "        print(f\"Using attention implementation: {attn_impl}\")\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        print(f\"Loading model from {model_path}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=attn_impl\n",
        "        )\n",
        "\n",
        "        # Ensure pad token exists\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model.eval()\n",
        "        self.num_layers = len(self.model.model.layers)\n",
        "\n",
        "        print(f\"✓ Model loaded with {self.num_layers} layers\")\n",
        "        print(f\"✓ Model parameters: {sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B\")\n",
        "\n",
        "        # Set up generation config\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_length=config.max_length,\n",
        "            temperature=config.temperature,\n",
        "            top_p=config.top_p,\n",
        "            do_sample=config.do_sample,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    def get_residual_stream(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None,\n",
        "                           past_key_values=None, use_cache: bool = True) -> Tuple[torch.Tensor, torch.Tensor, Any]:\n",
        "        \"\"\"\n",
        "        Forward pass that returns logits and the residual stream before final LayerNorm.\n",
        "\n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, vocab_size]\n",
        "            residual_stream: [batch_size, seq_len, hidden_size]\n",
        "            past_key_values: KV cache for next iteration\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Get embeddings\n",
        "            inputs_embeds = self.model.model.embed_tokens(input_ids)\n",
        "\n",
        "            # Add positional embeddings if needed\n",
        "            if hasattr(self.model.model, 'embed_positions'):\n",
        "                position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
        "                inputs_embeds += self.model.model.embed_positions(position_ids)\n",
        "\n",
        "            # Forward through transformer layers\n",
        "            hidden_states = inputs_embeds\n",
        "            new_past_key_values = [] if use_cache else None\n",
        "\n",
        "            for i, layer in enumerate(self.model.model.layers):\n",
        "                layer_past = past_key_values[i] if past_key_values else None\n",
        "\n",
        "                layer_outputs = layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    past_key_value=layer_past,\n",
        "                    use_cache=use_cache\n",
        "                )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if use_cache:\n",
        "                    new_past_key_values.append(layer_outputs[1])\n",
        "\n",
        "            # Store residual stream BEFORE final norm\n",
        "            residual_stream = hidden_states.clone()\n",
        "\n",
        "            # Apply final norm and get logits\n",
        "            if hasattr(self.model.model, 'norm'):\n",
        "                hidden_states = self.model.model.norm(hidden_states)\n",
        "\n",
        "            logits = self.model.lm_head(hidden_states)\n",
        "\n",
        "            return logits, residual_stream, new_past_key_values\n",
        "\n",
        "    def second_pass_forward(self, residual_stream: torch.Tensor, attention_mask: torch.Tensor = None,\n",
        "                           num_layers: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Second forward pass using residual stream as input.\n",
        "\n",
        "        Args:\n",
        "            residual_stream: [batch_size, seq_len, hidden_size]\n",
        "            attention_mask: attention mask\n",
        "            num_layers: number of layers to use (for partial passes)\n",
        "\n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Apply residual variant\n",
        "            if self.config.residual_variant == 'layernorm':\n",
        "                if hasattr(self.model.model, 'norm'):\n",
        "                    hidden_states = self.model.model.norm(residual_stream)\n",
        "                else:\n",
        "                    # Use a simple LayerNorm if model doesn't have norm\n",
        "                    hidden_states = F.layer_norm(residual_stream, residual_stream.shape[-1:])\n",
        "            else:\n",
        "                hidden_states = residual_stream\n",
        "\n",
        "            # Determine number of layers to use\n",
        "            layers_to_use = num_layers if num_layers is not None else self.num_layers\n",
        "            layers_to_use = min(layers_to_use, self.num_layers)\n",
        "\n",
        "            # Forward through specified number of layers\n",
        "            for i in range(layers_to_use):\n",
        "                layer_outputs = self.model.model.layers[i](\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    use_cache=False  # Don't use cache for second pass\n",
        "                )\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            # Apply final norm and get logits\n",
        "            if hasattr(self.model.model, 'norm'):\n",
        "                hidden_states = self.model.model.norm(hidden_states)\n",
        "\n",
        "            logits = self.model.lm_head(hidden_states)\n",
        "\n",
        "            return logits\n",
        "\n",
        "    def generate_with_double_pass(self, prompt: str, max_new_tokens: int = 100,\n",
        "                                 logger: Optional[ExpLogger] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate text using double forward pass strategy.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with generated text, token stats, and metadata\n",
        "        \"\"\"\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        # Initialize tracking\n",
        "        generated_tokens = []\n",
        "        token_stats = []\n",
        "        past_key_values = None\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            # First forward pass\n",
        "            logits1, residual_stream, past_key_values = self.get_residual_stream(\n",
        "                input_ids, attention_mask, past_key_values, use_cache=True\n",
        "            )\n",
        "\n",
        "            # Get logits for current position\n",
        "            current_logits1 = logits1[:, -1, :]  # [batch_size, vocab_size]\n",
        "\n",
        "            if self.config.pass_type == 'baseline':\n",
        "                # Use first pass logits directly\n",
        "                final_logits = current_logits1\n",
        "                entropy1 = self._compute_entropy(current_logits1)\n",
        "                entropy2 = entropy1  # Same for baseline\n",
        "            else:\n",
        "                # Second forward pass\n",
        "                num_layers = self.config.second_pass_layers if self.config.pass_type == 'double_partial' else None\n",
        "                logits2 = self.second_pass_forward(residual_stream, attention_mask, num_layers)\n",
        "                current_logits2 = logits2[:, -1, :]\n",
        "\n",
        "                # Use second pass logits for sampling\n",
        "                final_logits = current_logits2\n",
        "                entropy1 = self._compute_entropy(current_logits1)\n",
        "                entropy2 = self._compute_entropy(current_logits2)\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(final_logits / self.config.temperature, dim=-1)\n",
        "\n",
        "            if self.config.do_sample:\n",
        "                # Top-p sampling\n",
        "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above top_p\n",
        "                sorted_indices_to_remove = cumulative_probs > self.config.top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                probs[indices_to_remove] = 0\n",
        "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            # Record token statistics\n",
        "            token_stats.append({\n",
        "                'step': step,\n",
        "                'token_id': next_token.item(),\n",
        "                'token_text': self.tokenizer.decode(next_token.item()),\n",
        "                'entropy_pass1': entropy1.item(),\n",
        "                'entropy_pass2': entropy2.item(),\n",
        "                'top5_tokens_pass1': torch.topk(F.softmax(current_logits1, dim=-1), 5).indices.tolist(),\n",
        "                'top5_probs_pass1': torch.topk(F.softmax(current_logits1, dim=-1), 5).values.tolist(),\n",
        "                'top5_tokens_pass2': torch.topk(F.softmax(final_logits, dim=-1), 5).indices.tolist() if self.config.pass_type != 'baseline' else None,\n",
        "                'top5_probs_pass2': torch.topk(F.softmax(final_logits, dim=-1), 5).values.tolist() if self.config.pass_type != 'baseline' else None,\n",
        "            })\n",
        "\n",
        "            # Log to experiment logger if provided\n",
        "            if logger:\n",
        "                logger.log_token_stats({\n",
        "                    'prompt_hash': hashlib.md5(prompt.encode()).hexdigest()[:8],\n",
        "                    'step': step,\n",
        "                    'token_id': next_token.item(),\n",
        "                    'entropy_base': entropy1.item(),\n",
        "                    'entropy_double': entropy2.item(),\n",
        "                    'pass_type': self.config.pass_type\n",
        "                })\n",
        "\n",
        "            # Append token and continue\n",
        "            generated_tokens.append(next_token.item())\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "            # Update attention mask\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "            # Stop if EOS token\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Decode generated text\n",
        "        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        full_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'generated_text': generated_text,\n",
        "            'full_text': full_text,\n",
        "            'token_stats': token_stats,\n",
        "            'generation_time': generation_time,\n",
        "            'num_tokens': len(generated_tokens),\n",
        "            'config': self.config\n",
        "        }\n",
        "\n",
        "    def _compute_entropy(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute entropy of logits\"\"\"\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        entropy = -(probs * log_probs).sum(dim=-1)\n",
        "        return entropy.mean()  # Average over batch\n",
        "\n",
        "print(\"✓ DoublePassPhi3 model implementation loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a22808bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a22808bf",
        "outputId": "51c0a2d7-0712-475e-d10d-72deebd749c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ lm-eval integration loaded\n"
          ]
        }
      ],
      "source": [
        "# lm-eval integration for benchmarks\n",
        "\n",
        "class LMEvalWrapper:\n",
        "    \"\"\"Wrapper to integrate DoublePassPhi3 with lm-eval-harness\"\"\"\n",
        "\n",
        "    def __init__(self, double_pass_model: DoublePassPhi3):\n",
        "        self.model = double_pass_model\n",
        "        self.tokenizer = double_pass_model.tokenizer\n",
        "        self.device = double_pass_model.device\n",
        "\n",
        "    def loglikelihood(self, requests):\n",
        "        \"\"\"Compute log-likelihood for multiple choice tasks\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for context, continuation in requests:\n",
        "            # Tokenize context and continuation\n",
        "            context_tokens = self.tokenizer.encode(context, add_special_tokens=False)\n",
        "            continuation_tokens = self.tokenizer.encode(continuation, add_special_tokens=False)\n",
        "\n",
        "            # Full sequence\n",
        "            full_tokens = context_tokens + continuation_tokens\n",
        "            input_ids = torch.tensor([full_tokens], device=self.device)\n",
        "\n",
        "            # Get logits from model\n",
        "            with torch.no_grad():\n",
        "                if self.model.config.pass_type == 'baseline':\n",
        "                    # Standard forward pass\n",
        "                    outputs = self.model.model(input_ids)\n",
        "                    logits = outputs.logits\n",
        "                else:\n",
        "                    # Double pass\n",
        "                    logits1, residual_stream, _ = self.model.get_residual_stream(input_ids)\n",
        "                    if self.model.config.pass_type == 'double_partial':\n",
        "                        logits = self.model.second_pass_forward(\n",
        "                            residual_stream,\n",
        "                            num_layers=self.model.config.second_pass_layers\n",
        "                        )\n",
        "                    else:\n",
        "                        logits = self.model.second_pass_forward(residual_stream)\n",
        "\n",
        "                # Calculate log-likelihood for continuation tokens\n",
        "                shift_logits = logits[..., len(context_tokens)-1:-1, :].contiguous()\n",
        "                shift_labels = torch.tensor(continuation_tokens, device=self.device).unsqueeze(0)\n",
        "\n",
        "                log_probs = F.log_softmax(shift_logits, dim=-1)\n",
        "                log_likelihood = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1).sum().item()\n",
        "\n",
        "                # Return (log_likelihood, is_greedy)\n",
        "                results.append((log_likelihood, False))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_until(self, requests):\n",
        "        \"\"\"Generate text until stop sequences\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for context, gen_kwargs in requests:\n",
        "            # Extract generation parameters\n",
        "            max_gen_toks = gen_kwargs.get('max_gen_toks', 100)\n",
        "            until = gen_kwargs.get('until', [])\n",
        "\n",
        "            # Generate with double pass\n",
        "            result = self.model.generate_with_double_pass(\n",
        "                context,\n",
        "                max_new_tokens=max_gen_toks\n",
        "            )\n",
        "\n",
        "            generated = result['generated_text']\n",
        "\n",
        "            # Apply stop sequences\n",
        "            for stop_seq in until:\n",
        "                if stop_seq in generated:\n",
        "                    generated = generated.split(stop_seq)[0]\n",
        "                    break\n",
        "\n",
        "            results.append(generated)\n",
        "\n",
        "        return results\n",
        "\n",
        "def run_benchmark(model_wrapper: LMEvalWrapper, benchmark_name: str,\n",
        "                 config: ExperimentConfig, logger: ExpLogger) -> Dict[str, Any]:\n",
        "    \"\"\"Run a specific benchmark using lm-eval\"\"\"\n",
        "\n",
        "    print(f\"Running {benchmark_name} benchmark...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Map benchmark names to lm-eval task names\n",
        "    task_mapping = {\n",
        "        'mmlu': 'mmlu',\n",
        "        'bbh': 'bigbench_hard',\n",
        "        'gsm8k': 'gsm8k'\n",
        "    }\n",
        "\n",
        "    task_name = task_mapping.get(benchmark_name.lower(), benchmark_name)\n",
        "\n",
        "    try:\n",
        "        # Import lm-eval\n",
        "        from lm_eval import evaluator\n",
        "        from lm_eval.models.huggingface import HFLM\n",
        "\n",
        "        # Create a custom model class that uses our wrapper\n",
        "        class CustomHFLM(HFLM):\n",
        "            def __init__(self, wrapper):\n",
        "                self.wrapper = wrapper\n",
        "                self.tokenizer = wrapper.tokenizer\n",
        "                self.device = wrapper.device\n",
        "\n",
        "            def loglikelihood(self, requests):\n",
        "                return self.wrapper.loglikelihood(requests)\n",
        "\n",
        "            def generate_until(self, requests):\n",
        "                return self.wrapper.generate_until(requests)\n",
        "\n",
        "        # Create model instance\n",
        "        model = CustomHFLM(model_wrapper)\n",
        "\n",
        "        # Run evaluation\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=model,\n",
        "            tasks=[task_name],\n",
        "            num_fewshot=5 if benchmark_name.lower() in ['mmlu', 'gsm8k'] else 0,\n",
        "            batch_size=1,  # Keep small for memory\n",
        "            device=str(model_wrapper.device),\n",
        "            no_cache=True\n",
        "        )\n",
        "\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        # Extract results\n",
        "        task_results = results['results'][task_name]\n",
        "        accuracy = task_results.get('acc', task_results.get('exact_match', 0.0))\n",
        "\n",
        "        # Log results\n",
        "        result_dict = {\n",
        "            'benchmark': benchmark_name,\n",
        "            'variant': f\"{config.pass_type}_{config.second_pass_layers if config.second_pass_layers else 'full'}_{config.residual_variant}\",\n",
        "            'accuracy': accuracy,\n",
        "            'num_samples': task_results.get('num_samples', 0),\n",
        "            'avg_entropy_base': 0.0,  # Will be computed from token stats\n",
        "            'avg_entropy_double': 0.0,  # Will be computed from token stats\n",
        "            'runtime_sec': runtime,\n",
        "            'tokens_generated': 0,  # Will be computed from token stats\n",
        "            'compute_overhead': 2.0 if config.pass_type != 'baseline' else 1.0\n",
        "        }\n",
        "\n",
        "        logger.log_results(result_dict)\n",
        "        logger.log_scalar(f\"{benchmark_name}/accuracy\", accuracy, 0)\n",
        "\n",
        "        print(f\"✓ {benchmark_name} completed: {accuracy:.3f} accuracy in {runtime:.1f}s\")\n",
        "\n",
        "        return {\n",
        "            'benchmark': benchmark_name,\n",
        "            'accuracy': accuracy,\n",
        "            'runtime': runtime,\n",
        "            'full_results': task_results\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error running {benchmark_name}: {str(e)}\")\n",
        "        return {\n",
        "            'benchmark': benchmark_name,\n",
        "            'accuracy': 0.0,\n",
        "            'runtime': 0.0,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "print(\"✓ lm-eval integration loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "80fe4288",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80fe4288",
        "outputId": "3c4c16ec-1435-411d-98da-ff3f21a56dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Experiment runner loaded\n"
          ]
        }
      ],
      "source": [
        "# Experiment runner and main loop\n",
        "\n",
        "def run_experiment(config: ExperimentConfig, benchmarks: List[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Run a complete experiment with the given configuration\"\"\"\n",
        "\n",
        "    if benchmarks is None:\n",
        "        benchmarks = ['mmlu', 'bbh', 'gsm8k']\n",
        "\n",
        "    # Create logger\n",
        "    run_dir = f\"runs/{config.timestamp}-{config.pass_type}\"\n",
        "    if config.second_pass_layers:\n",
        "        run_dir += f\"-{config.second_pass_layers}layers\"\n",
        "    run_dir += f\"-{config.residual_variant}\"\n",
        "\n",
        "    logger = ExpLogger(run_dir)\n",
        "    logger.save_config(config)\n",
        "\n",
        "    print(f\"Starting experiment: {config.pass_type}\")\n",
        "    print(f\"Logs will be saved to: {run_dir}\")\n",
        "\n",
        "    # Initialize model\n",
        "    try:\n",
        "        model = DoublePassPhi3(config.model_path, config)\n",
        "        wrapper = LMEvalWrapper(model)\n",
        "\n",
        "        # Run benchmarks\n",
        "        results = {}\n",
        "        for benchmark in benchmarks:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Running {benchmark.upper()} benchmark\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            result = run_benchmark(wrapper, benchmark, config, logger)\n",
        "            results[benchmark] = result\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            logger.log_scalar(f\"accuracy/{benchmark}\", result['accuracy'], 0)\n",
        "            logger.log_scalar(f\"runtime/{benchmark}\", result['runtime'], 0)\n",
        "\n",
        "        # Compute aggregate metrics\n",
        "        total_accuracy = np.mean([r['accuracy'] for r in results.values()])\n",
        "        total_runtime = sum([r['runtime'] for r in results.values()])\n",
        "\n",
        "        logger.log_scalar(\"accuracy/overall\", total_accuracy, 0)\n",
        "        logger.log_scalar(\"runtime/total\", total_runtime, 0)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EXPERIMENT COMPLETE\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Overall accuracy: {total_accuracy:.3f}\")\n",
        "        print(f\"Total runtime: {total_runtime:.1f}s\")\n",
        "        print(f\"Results saved to: {run_dir}\")\n",
        "\n",
        "        logger.finalize()\n",
        "\n",
        "        return {\n",
        "            'config': config,\n",
        "            'results': results,\n",
        "            'overall_accuracy': total_accuracy,\n",
        "            'total_runtime': total_runtime,\n",
        "            'run_dir': run_dir\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Experiment failed: {str(e)}\")\n",
        "        logger.finalize()\n",
        "        raise e\n",
        "\n",
        "def run_all_experiments(model_path: str, benchmarks: List[str] = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Run all experimental conditions\"\"\"\n",
        "\n",
        "    if benchmarks is None:\n",
        "        benchmarks = ['mmlu', 'bbh', 'gsm8k']\n",
        "\n",
        "    # Define experimental conditions\n",
        "    experiments = []\n",
        "\n",
        "    # 1. Baseline\n",
        "    experiments.append(ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='baseline',\n",
        "        residual_variant='raw'\n",
        "    ))\n",
        "\n",
        "    # 2. Double-pass full\n",
        "    experiments.append(ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='double_full',\n",
        "        residual_variant='raw'\n",
        "    ))\n",
        "\n",
        "    # 3. Double-pass with LayerNorm\n",
        "    experiments.append(ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='double_full',\n",
        "        residual_variant='layernorm'\n",
        "    ))\n",
        "\n",
        "    # 4. Partial double-pass (different layer counts)\n",
        "    for num_layers in [1, 2, 4, 8]:\n",
        "        experiments.append(ExperimentConfig(\n",
        "            model_path=model_path,\n",
        "            pass_type='double_partial',\n",
        "            second_pass_layers=num_layers,\n",
        "            residual_variant='raw'\n",
        "        ))\n",
        "\n",
        "    print(f\"Running {len(experiments)} experiments across {len(benchmarks)} benchmarks\")\n",
        "    print(f\"Estimated total time: {len(experiments) * len(benchmarks) * 30 / 60:.1f} minutes\")\n",
        "\n",
        "    # Run all experiments\n",
        "    all_results = []\n",
        "    for i, config in enumerate(experiments):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EXPERIMENT {i+1}/{len(experiments)}\")\n",
        "        print(f\"Config: {config.pass_type}, layers: {config.second_pass_layers}, variant: {config.residual_variant}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            result = run_experiment(config, benchmarks)\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Clear GPU cache between experiments\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Experiment {i+1} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ALL EXPERIMENTS COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Completed {len(all_results)}/{len(experiments)} experiments\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "print(\"✓ Experiment runner loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "42082695",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42082695",
        "outputId": "5e420687-dd42-4dff-d22d-d36f7e6286b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running quick tests...\n",
            "Testing model loading...\n",
            "Using attention implementation: sdpa\n",
            "Loading model from /content/drive/MyDrive/phi3_3.8B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✗ Model loading failed: Phi3ForCausalLM does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet. Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`\n",
            "✗ Tests failed - check model path and setup\n"
          ]
        }
      ],
      "source": [
        "# Quick test and demo cell\n",
        "\n",
        "def test_model_loading():\n",
        "    \"\"\"Test that the model loads correctly\"\"\"\n",
        "    print(\"Testing model loading...\")\n",
        "\n",
        "    config = ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='baseline'\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model = DoublePassPhi3(model_path, config)\n",
        "        print(\"✓ Model loaded successfully\")\n",
        "        print(f\"  - Layers: {model.num_layers}\")\n",
        "        print(f\"  - Parameters: {sum(p.numel() for p in model.model.parameters()) / 1e9:.1f}B\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Model loading failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def demo_generation():\n",
        "    \"\"\"Demo generation with different pass types\"\"\"\n",
        "    print(\"\\nTesting generation with different pass types...\")\n",
        "\n",
        "    test_prompts = [\n",
        "        \"The capital of France is\",\n",
        "        \"2 + 2 equals\",\n",
        "        \"The largest planet in our solar system is\"\n",
        "    ]\n",
        "\n",
        "    # Test baseline\n",
        "    print(\"\\n--- BASELINE ---\")\n",
        "    config_baseline = ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='baseline',\n",
        "        attn_impl='auto',  # Auto-detect: Flash Attention 2 if available, else SDPA\n",
        "        temperature=0.7,\n",
        "        max_length=50\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model_baseline = DoublePassPhi3(model_path, config_baseline)\n",
        "\n",
        "        for prompt in test_prompts:\n",
        "            result = model_baseline.generate_with_double_pass(prompt, max_new_tokens=20)\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            print(f\"Output: {result['generated_text']}\")\n",
        "            print(f\"Tokens: {result['num_tokens']}, Time: {result['generation_time']:.2f}s\")\n",
        "            print()\n",
        "\n",
        "        del model_baseline\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Baseline generation failed: {str(e)}\")\n",
        "\n",
        "    # Test double-pass\n",
        "    print(\"\\n--- DOUBLE PASS ---\")\n",
        "    config_double = ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='double_full',\n",
        "        attn_impl='auto',  # Auto-detect: Flash Attention 2 if available, else SDPA\n",
        "        temperature=0.7,\n",
        "        max_length=50\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model_double = DoublePassPhi3(model_path, config_double)\n",
        "\n",
        "        for prompt in test_prompts:\n",
        "            result = model_double.generate_with_double_pass(prompt, max_new_tokens=20)\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            print(f\"Output: {result['generated_text']}\")\n",
        "            print(f\"Tokens: {result['num_tokens']}, Time: {result['generation_time']:.2f}s\")\n",
        "\n",
        "            # Show entropy comparison\n",
        "            if result['token_stats']:\n",
        "                avg_entropy1 = np.mean([s['entropy_pass1'] for s in result['token_stats']])\n",
        "                avg_entropy2 = np.mean([s['entropy_pass2'] for s in result['token_stats']])\n",
        "                print(f\"Avg entropy - Pass 1: {avg_entropy1:.3f}, Pass 2: {avg_entropy2:.3f}\")\n",
        "            print()\n",
        "\n",
        "        del model_double\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Double-pass generation failed: {str(e)}\")\n",
        "\n",
        "# Run tests\n",
        "print(\"Running quick tests...\")\n",
        "test_model = test_model_loading()\n",
        "if test_model:\n",
        "    demo_generation()\n",
        "    print(\"✓ All tests completed successfully!\")\n",
        "else:\n",
        "    print(\"✗ Tests failed - check model path and setup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U4kSFO265Q9m",
      "metadata": {
        "id": "U4kSFO265Q9m"
      },
      "outputs": [],
      "source": [
        "## 🚀 Run Full Experiments\n",
        "\n",
        "**Important**: The cell below will run all experiments. This will take several hours on an A100 GPU.\n",
        "\n",
        "**Before running**:\n",
        "1. Make sure the model loaded successfully in the test above\n",
        "2. Ensure you have sufficient GPU memory\n",
        "3. Consider running a single experiment first to verify everything works\n",
        "\n",
        "**What will happen**:\n",
        "- 7 different experimental conditions will be tested\n",
        "- Each will be evaluated on MMLU, BigBench-Hard, and GSM8K\n",
        "- Results will be saved to local `runs/` directory\n",
        "- TensorBoard logs will be created for visualization\n",
        "\n",
        "**Estimated time**: 6-10 hours total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4737aea6",
      "metadata": {
        "id": "4737aea6"
      },
      "outputs": [],
      "source": [
        "# Run all experiments\n",
        "# WARNING: This will take several hours!\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Run all experiments\n",
        "print(\"Starting full experimental suite...\")\n",
        "print(\"This will take several hours - grab a coffee (or several)!\")\n",
        "\n",
        "try:\n",
        "    all_results = run_all_experiments(model_path)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for result in all_results:\n",
        "        config = result['config']\n",
        "        variant_name = f\"{config.pass_type}\"\n",
        "        if config.second_pass_layers:\n",
        "            variant_name += f\"_{config.second_pass_layers}layers\"\n",
        "        variant_name += f\"_{config.residual_variant}\"\n",
        "\n",
        "        print(f\"\\n{variant_name}:\")\n",
        "        print(f\"  Overall accuracy: {result['overall_accuracy']:.3f}\")\n",
        "        print(f\"  Runtime: {result['total_runtime']:.1f}s\")\n",
        "\n",
        "        for benchmark, bench_result in result['results'].items():\n",
        "            print(f\"  {benchmark}: {bench_result['accuracy']:.3f}\")\n",
        "\n",
        "    print(f\"\\nAll results saved to individual run directories in 'runs/'\")\n",
        "    print(\"Use TensorBoard to visualize: tensorboard --logdir runs/\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Experiment suite failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab24757",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab24757",
        "outputId": "1f06ebd5-cf62-4831-88cc-60a6c424e4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running single experiment test...\n",
            "Starting experiment: double_full\n",
            "Logs will be saved to: runs/20250705_233613-double_full-raw\n",
            "Loading model from /content/drive/MyDrive/phi3_3.8B...\n",
            "✗ Experiment failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
            "Single experiment test failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-11-229376830.py\", line 22, in run_single_experiment_test\n",
            "    result = run_experiment(config, benchmarks)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-6-640186334.py\", line 67, in run_experiment\n",
            "    raise e\n",
            "  File \"/tmp/ipython-input-6-640186334.py\", line 23, in run_experiment\n",
            "    model = DoublePassPhi3(config.model_path, config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4-234219190.py\", line 20, in __init__\n",
            "    self.model = AutoModelForCausalLM.from_pretrained(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 593, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4752, in from_pretrained\n",
            "    config = cls._autoset_attn_implementation(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2315, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2457, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ],
      "source": [
        "# Alternative: Run single experiment for testing\n",
        "# Use this cell to test a single configuration before running the full suite\n",
        "\n",
        "def run_single_experiment_test():\n",
        "    \"\"\"Run a single experiment for testing purposes\"\"\"\n",
        "\n",
        "    print(\"Running single experiment test...\")\n",
        "\n",
        "    # Choose a fast configuration for testing\n",
        "    config = ExperimentConfig(\n",
        "        model_path=model_path,\n",
        "        pass_type='double_full',  # or 'baseline' for faster testing\n",
        "        residual_variant='raw',\n",
        "        attn_impl='auto',  # Auto-detect: Flash Attention 2 if available, else SDPA\n",
        "        temperature=0.7,\n",
        "        max_length=512  # Shorter for testing\n",
        "    )\n",
        "\n",
        "    # Run just one benchmark for testing\n",
        "    benchmarks = ['mmlu']  # Start with just MMLU\n",
        "\n",
        "    try:\n",
        "        result = run_experiment(config, benchmarks)\n",
        "        print(f\"\\nTest completed successfully!\")\n",
        "        print(f\"MMLU accuracy: {result['results']['mmlu']['accuracy']:.3f}\")\n",
        "        print(f\"Runtime: {result['total_runtime']:.1f}s\")\n",
        "        print(f\"Results saved to: {result['run_dir']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Single experiment test failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Uncomment the line below to run a single experiment test\n",
        "test_result = run_single_experiment_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8880a6ec",
      "metadata": {
        "id": "8880a6ec"
      },
      "outputs": [],
      "source": [
        "# Analysis and visualization utilities\n",
        "\n",
        "def analyze_results(runs_dir: str = \"runs/\"):\n",
        "    \"\"\"Analyze results from all completed experiments\"\"\"\n",
        "\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Collect all results.csv files\n",
        "    results_files = list(Path(runs_dir).glob(\"*/results.csv\"))\n",
        "\n",
        "    if not results_files:\n",
        "        print(\"No results found. Run experiments first!\")\n",
        "        return\n",
        "\n",
        "    # Load all results\n",
        "    all_data = []\n",
        "    for file in results_files:\n",
        "        df = pd.read_csv(file)\n",
        "        run_name = file.parent.name\n",
        "        df['run_name'] = run_name\n",
        "        all_data.append(df)\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    print(f\"Found {len(combined_df)} benchmark results across {len(results_files)} runs\")\n",
        "\n",
        "    # Create summary plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Accuracy by variant\n",
        "    accuracy_by_variant = combined_df.groupby('variant')['accuracy'].mean().sort_values(ascending=False)\n",
        "    axes[0, 0].bar(range(len(accuracy_by_variant)), accuracy_by_variant.values)\n",
        "    axes[0, 0].set_xticks(range(len(accuracy_by_variant)))\n",
        "    axes[0, 0].set_xticklabels(accuracy_by_variant.index, rotation=45, ha='right')\n",
        "    axes[0, 0].set_title('Average Accuracy by Variant')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "\n",
        "    # 2. Runtime by variant\n",
        "    runtime_by_variant = combined_df.groupby('variant')['runtime_sec'].mean()\n",
        "    axes[0, 1].bar(range(len(runtime_by_variant)), runtime_by_variant.values)\n",
        "    axes[0, 1].set_xticks(range(len(runtime_by_variant)))\n",
        "    axes[0, 1].set_xticklabels(runtime_by_variant.index, rotation=45, ha='right')\n",
        "    axes[0, 1].set_title('Average Runtime by Variant')\n",
        "    axes[0, 1].set_ylabel('Runtime (seconds)')\n",
        "\n",
        "    # 3. Accuracy by benchmark\n",
        "    benchmark_pivot = combined_df.pivot_table(values='accuracy', index='benchmark', columns='variant', aggfunc='mean')\n",
        "    benchmark_pivot.plot(kind='bar', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Accuracy by Benchmark and Variant')\n",
        "    axes[1, 0].set_ylabel('Accuracy')\n",
        "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # 4. Compute overhead vs accuracy\n",
        "    axes[1, 1].scatter(combined_df['compute_overhead'], combined_df['accuracy'], alpha=0.6)\n",
        "    axes[1, 1].set_xlabel('Compute Overhead')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].set_title('Accuracy vs Compute Overhead')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{runs_dir}/analysis_summary.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"\\nBest performing variant:\")\n",
        "    best_variant = accuracy_by_variant.index[0]\n",
        "    best_accuracy = accuracy_by_variant.iloc[0]\n",
        "    print(f\"  {best_variant}: {best_accuracy:.3f} accuracy\")\n",
        "\n",
        "    print(f\"\\nBaseline performance:\")\n",
        "    baseline_results = combined_df[combined_df['variant'].str.contains('baseline')]\n",
        "    if not baseline_results.empty:\n",
        "        baseline_acc = baseline_results['accuracy'].mean()\n",
        "        print(f\"  Baseline: {baseline_acc:.3f} accuracy\")\n",
        "\n",
        "        improvement = best_accuracy - baseline_acc\n",
        "        print(f\"  Best improvement: +{improvement:.3f} accuracy ({improvement/baseline_acc*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nRuntime comparison:\")\n",
        "    baseline_runtime = combined_df[combined_df['variant'].str.contains('baseline')]['runtime_sec'].mean()\n",
        "    double_runtime = combined_df[combined_df['variant'].str.contains('double')]['runtime_sec'].mean()\n",
        "    if baseline_runtime > 0 and double_runtime > 0:\n",
        "        overhead = double_runtime / baseline_runtime\n",
        "        print(f\"  Baseline: {baseline_runtime:.1f}s\")\n",
        "        print(f\"  Double-pass: {double_runtime:.1f}s\")\n",
        "        print(f\"  Overhead: {overhead:.1f}x\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def launch_tensorboard(runs_dir: str = \"runs/\"):\n",
        "    \"\"\"Launch TensorBoard to visualize results\"\"\"\n",
        "\n",
        "    print(f\"Launching TensorBoard for {runs_dir}\")\n",
        "    print(\"Note: In Colab, you may need to use the public URL\")\n",
        "\n",
        "    # Install tensorboard extension for Colab\n",
        "    try:\n",
        "        get_ipython().system('pip install -q tensorboard-plugin-profile')\n",
        "\n",
        "        # Load tensorboard extension\n",
        "        get_ipython().run_line_magic('load_ext', 'tensorboard')\n",
        "\n",
        "        # Launch tensorboard\n",
        "        get_ipython().run_line_magic('tensorboard', f'--logdir {runs_dir}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not launch TensorBoard in notebook: {str(e)}\")\n",
        "        print(f\"Run manually: tensorboard --logdir {runs_dir}\")\n",
        "\n",
        "# Uncomment to run analysis after experiments complete\n",
        "# results_df = analyze_results()\n",
        "# launch_tensorboard()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63599fd",
      "metadata": {
        "id": "a63599fd"
      },
      "outputs": [],
      "source": [
        "# Attention Implementation Demo\n",
        "\n",
        "def demo_attention_implementations():\n",
        "    \"\"\"Demo different attention implementations to verify fallback works\"\"\"\n",
        "\n",
        "    print(\"Testing different attention implementations...\")\n",
        "    print(\"This verifies that the automatic fallback from Flash Attention to SDPA works correctly.\\n\")\n",
        "\n",
        "    test_prompt = \"The capital of France is\"\n",
        "\n",
        "    # Test different attention implementations\n",
        "    attention_configs = [\n",
        "        (\"Auto-detect (recommended)\", \"auto\"),\n",
        "        (\"Force SDPA (safe fallback)\", \"sdpa\"),\n",
        "        (\"Force Eager (slowest but reliable)\", \"eager\"),\n",
        "        (\"Force Flash Attention 2 (will fail if not available)\", \"flash2\")\n",
        "    ]\n",
        "\n",
        "    for name, attn_impl in attention_configs:\n",
        "        print(f\"--- {name} ---\")\n",
        "\n",
        "        config = ExperimentConfig(\n",
        "            model_path=model_path,\n",
        "            pass_type='baseline',  # Use baseline for faster testing\n",
        "            attn_impl=attn_impl,\n",
        "            temperature=0.7,\n",
        "            max_length=100\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            model = DoublePassPhi3(model_path, config)\n",
        "            load_time = time.time() - start_time\n",
        "\n",
        "            # Quick generation test\n",
        "            result = model.generate_with_double_pass(test_prompt, max_new_tokens=10)\n",
        "\n",
        "            print(f\"✓ Success! Load time: {load_time:.2f}s\")\n",
        "            print(f\"  Generated: {result['generated_text']}\")\n",
        "            print(f\"  Generation time: {result['generation_time']:.3f}s\")\n",
        "\n",
        "            # Clean up\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed: {str(e)}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "# Uncomment to test attention implementations\n",
        "print(\"Available attention implementation options:\")\n",
        "print(\"- 'auto': Automatically detect Flash Attention 2, fallback to SDPA\")\n",
        "print(\"- 'flash2': Force Flash Attention 2 (fails if not available)\")\n",
        "print(\"- 'sdpa': Force PyTorch SDPA (fast, reliable)\")\n",
        "print(\"- 'eager': Force eager attention (slowest but most compatible)\")\n",
        "print(\"\\nRecommendation: Use 'auto' for automatic fallback behavior\")\n",
        "\n",
        "# demo_attention_implementations()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CeMGIT8L5Q9n",
      "metadata": {
        "id": "CeMGIT8L5Q9n"
      },
      "outputs": [],
      "source": [
        "## 📊 Interactive Demo & Exploration\n",
        "\n",
        "Use the cells below to interactively explore the double-pass generation and compare outputs between different configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4697669f",
      "metadata": {
        "id": "4697669f"
      },
      "outputs": [],
      "source": [
        "# Interactive prompt testing\n",
        "\n",
        "def interactive_comparison(prompt: str, max_tokens: int = 50):\n",
        "    \"\"\"Compare baseline vs double-pass generation for a given prompt\"\"\"\n",
        "\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    configs = [\n",
        "        (\"Baseline\", ExperimentConfig(model_path=model_path, pass_type='baseline')),\n",
        "        (\"Double-Pass Full\", ExperimentConfig(model_path=model_path, pass_type='double_full')),\n",
        "        (\"Double-Pass 4 Layers\", ExperimentConfig(model_path=model_path, pass_type='double_partial', second_pass_layers=4)),\n",
        "        (\"Double-Pass + LayerNorm\", ExperimentConfig(model_path=model_path, pass_type='double_full', residual_variant='layernorm'))\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, config in configs:\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        try:\n",
        "            model = DoublePassPhi3(model_path, config)\n",
        "            result = model.generate_with_double_pass(prompt, max_new_tokens=max_tokens)\n",
        "\n",
        "            print(f\"Generated: {result['generated_text']}\")\n",
        "            print(f\"Time: {result['generation_time']:.2f}s\")\n",
        "            print(f\"Tokens: {result['num_tokens']}\")\n",
        "\n",
        "            if result['token_stats']:\n",
        "                entropies1 = [s['entropy_pass1'] for s in result['token_stats']]\n",
        "                entropies2 = [s['entropy_pass2'] for s in result['token_stats']]\n",
        "                print(f\"Avg entropy pass 1: {np.mean(entropies1):.3f}\")\n",
        "                print(f\"Avg entropy pass 2: {np.mean(entropies2):.3f}\")\n",
        "\n",
        "                # Show token-by-token comparison for first few tokens\n",
        "                print(\"Token details (first 5):\")\n",
        "                for i, stats in enumerate(result['token_stats'][:5]):\n",
        "                    print(f\"  {i+1}. '{stats['token_text']}' - H1: {stats['entropy_pass1']:.3f}, H2: {stats['entropy_pass2']:.3f}\")\n",
        "\n",
        "            results[name] = result\n",
        "\n",
        "            # Clean up\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            results[name] = None\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test prompts for different reasoning types\n",
        "test_prompts = [\n",
        "    \"The largest city in Australia is\",\n",
        "    \"If I have 3 apples and give away 2, how many do I have left?\",\n",
        "    \"The chemical symbol for gold is\",\n",
        "    \"What is the square root of 64?\",\n",
        "    \"Complete this sequence: 2, 4, 8, 16,\",\n",
        "    \"The author of '1984' is\"\n",
        "]\n",
        "\n",
        "print(\"Available test prompts:\")\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"{i+1}. {prompt}\")\n",
        "\n",
        "# Example usage:\n",
        "# results = interactive_comparison(\"The largest city in Australia is\", max_tokens=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-nBF-fBy97S"
      },
      "outputs": [],
      "source": [
        "## 🔧 Flash Attention Compatibility Fix\n",
        "\n",
        "### Problem Solved\n",
        "This notebook now handles the Flash Attention 2 / PyTorch 2.6 compatibility issue automatically:\n",
        "\n",
        "- **Before**: Hard-coded `flash_attention_2` caused crashes when FA2 wasn't compatible\n",
        "- **After**: Intelligent fallback system that detects available attention implementations\n",
        "\n",
        "### New Attention Parameter\n",
        "All `ExperimentConfig` instances now support an `attn_impl` parameter:\n",
        "\n",
        "```python\n",
        "config = ExperimentConfig(\n",
        "    model_path=model_path,\n",
        "    pass_type='baseline',\n",
        "    attn_impl='auto',  # ← New parameter\n",
        "    # ... other params\n",
        ")\n",
        "```\n",
        "\n",
        "### Attention Implementation Options\n",
        "\n",
        "| Option | Description | When to Use |\n",
        "|--------|-------------|-------------|\n",
        "| `'auto'` | **Recommended**. Auto-detect Flash Attention 2, fallback to SDPA | Default for all use cases |\n",
        "| `'sdpa'` | Force PyTorch SDPA (Scaled Dot Product Attention) | When you want consistent behavior |\n",
        "| `'eager'` | Force basic eager attention | Debugging or maximum compatibility |\n",
        "| `'flash2'` | Force Flash Attention 2 | Only when you know FA2 is available |\n",
        "\n",
        "### Performance Impact\n",
        "- **Flash Attention 2**: Fastest (~1.0x baseline)\n",
        "- **SDPA**: Good performance (~1.3-1.5x slower than FA2)\n",
        "- **Eager**: Slowest (~2-3x slower than FA2)\n",
        "\n",
        "For research purposes, the performance difference between FA2 and SDPA is usually acceptable.\n"
      ],
      "id": "j-nBF-fBy97S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zao8JW295Q9o",
      "metadata": {
        "id": "Zao8JW295Q9o"
      },
      "outputs": [],
      "source": [
        "## 📝 Usage Instructions\n",
        "\n",
        "### Quick Start\n",
        "1. **Run the setup cells** (1-4) to install packages and load utilities\n",
        "2. **Test model loading** with the quick test cell\n",
        "3. **Try interactive demo** with a few prompts to verify everything works\n",
        "4. **Run single experiment** to test one configuration\n",
        "5. **Run full experiment suite** (will take 6-10 hours)\n",
        "6. **Analyze results** using the analysis utilities\n",
        "\n",
        "### File Structure\n",
        "After running experiments, you'll have:\n",
        "```\n",
        "runs/\n",
        "├── YYYYMMDD_HHMMSS-baseline-raw/\n",
        "│   ├── run_config.json\n",
        "│   ├── token_stats.jsonl\n",
        "│   ├── results.csv\n",
        "│   ├── tb/                # TensorBoard logs\n",
        "│   └── summary.txt\n",
        "├── YYYYMMDD_HHMMSS-double_full-raw/\n",
        "│   └── ...\n",
        "└── analysis_summary.png   # Generated by analysis\n",
        "```\n",
        "\n",
        "### Key Functions\n",
        "- `interactive_comparison(prompt)` - Compare different configurations on a prompt\n",
        "- `run_single_experiment_test()` - Test one configuration quickly\n",
        "- `run_all_experiments()` - Run full experimental suite\n",
        "- `analyze_results()` - Generate summary plots and statistics\n",
        "- `launch_tensorboard()` - View detailed logs\n",
        "\n",
        "### Expected Results\n",
        "- **Hypothesis**: Double-pass should reduce entropy and improve accuracy\n",
        "- **Metrics**: Track accuracy, entropy, runtime, compute overhead\n",
        "- **Benchmarks**: MMLU, BigBench-Hard, GSM8K\n",
        "- **Variants**: Baseline, full double-pass, partial double-pass, LayerNorm variants\n",
        "\n",
        "### Flash Attention Compatibility\n",
        "- **Fixed**: Automatic fallback from Flash Attention 2 to SDPA when FA2 isn't available\n",
        "- **Default**: All configs use `attn_impl='auto'` for automatic detection\n",
        "- **Performance**: SDPA is ~1.3x slower than FA2 but much faster than eager attention\n",
        "- **Override**: Set `attn_impl='sdpa'` or `attn_impl='eager'` to force specific implementations\n",
        "\n",
        "### Troubleshooting\n",
        "- ✅ **Flash Attention errors**: Now automatically handled with SDPA fallback\n",
        "- If model fails to load, check the path `/content/drive/MyDrive/phi3_3.8B`\n",
        "- If out of memory, reduce `max_length` in configs\n",
        "- If lm-eval fails, try installing older version: `pip install lm-eval==0.3.0`\n",
        "- For TensorBoard issues, run manually: `tensorboard --logdir runs/`\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}