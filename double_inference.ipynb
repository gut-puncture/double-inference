{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/double-inference/blob/main/double_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAJU3CLZdGI3",
        "outputId": "90dd82db-87cf-4021-bd80-3ed3b1f77223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "VRAM: 42.5 GB\n",
            "ExperimentConfig(model_path='/content/drive/MyDrive/phi3_3.8B', max_length=2048, temperature=0.7, top_p=0.9, seed=42, pass_type='baseline', second_pass_layers=None, residual_variant='raw', attn_impl='auto', entropy_eps=1e-09, min_vram_gb=10.0)\n"
          ]
        }
      ],
      "source": [
        "### 1Ô∏è‚É£ Setup & Config\n",
        "\n",
        "# Fix version conflicts by installing compatible versions\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.44.0 accelerate datasets==2.18.0\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# All imports consolidated (removed lm-eval to avoid conflicts)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# GPU check\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cpu': raise ValueError('No GPU available')\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f'GPU: {gpu_name}')\n",
        "print(f'VRAM: {vram_gb:.1f} GB')\n",
        "\n",
        "# Config with constants\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    model_path: str = '/content/drive/MyDrive/phi3_3.8B'\n",
        "    max_length: int = 2048\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "    seed: int = 42\n",
        "    pass_type: str = 'baseline'\n",
        "    second_pass_layers: int = None\n",
        "    residual_variant: str = 'raw'\n",
        "    attn_impl: str = 'auto'\n",
        "    entropy_eps: float = 1e-9  # New constant\n",
        "    min_vram_gb: float = 10.0  # New constant\n",
        "\n",
        "config = ExperimentConfig()\n",
        "if vram_gb < config.min_vram_gb: raise ValueError('Insufficient VRAM')\n",
        "torch.manual_seed(config.seed)\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZATN8NdXdGI4"
      },
      "outputs": [],
      "source": [
        "### 2Ô∏è‚É£ Utility ‚Äì Attention Implementation Fallback\n",
        "\n",
        "def get_fallback_chain(requested: str) -> List[str]:\n",
        "    # Always use eager attention for Phi-3 compatibility\n",
        "    return ['eager']\n",
        "\n",
        "def resolve_attn_impl(requested: str) -> str:\n",
        "    return get_fallback_chain(requested)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8wT0SLEUdGI5"
      },
      "outputs": [],
      "source": [
        "### 3Ô∏è‚É£ Utility ‚Äì Robust Embedding Builder\n",
        "\n",
        "def build_inputs_embeds(model, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "    token_embeds = model.get_input_embeddings()(input_ids)\n",
        "    seq_len = input_ids.size(1)\n",
        "    position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "    pos_attrs = [\n",
        "        ('model.embed_positions', lambda m: getattr(m.model, 'embed_positions', None)),\n",
        "        ('wpe', lambda m: getattr(m.transformer, 'wpe', None) if hasattr(m, 'transformer') else None),\n",
        "        ('embeddings.position_embeddings', lambda m: getattr(m.embeddings, 'position_embeddings', None) if hasattr(m, 'embeddings') else None)\n",
        "    ]\n",
        "    for _, getter in pos_attrs:\n",
        "        pos_layer = getter(model)\n",
        "        if pos_layer is not None and callable(pos_layer):\n",
        "            pos_embeds = pos_layer(position_ids)\n",
        "            if pos_embeds is not None and pos_embeds.shape[:2] == token_embeds.shape[:2]:\n",
        "                token_embeds += pos_embeds\n",
        "                break\n",
        "    return token_embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1NiC5xljdGI6"
      },
      "outputs": [],
      "source": [
        "### 4Ô∏è‚É£ Class DoublePassPhi3\n",
        "\n",
        "class DoublePassPhi3(torch.nn.Module):\n",
        "    def __init__(self, model_path: str, config: ExperimentConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda')\n",
        "        attn_impl = resolve_attn_impl(config.attn_impl)\n",
        "\n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path, torch_dtype=torch.float16, device_map='auto',\n",
        "                trust_remote_code=True, attn_implementation=attn_impl\n",
        "            )\n",
        "            print(f'‚úÖ Model loaded successfully with {attn_impl} attention')\n",
        "        except Exception as e:\n",
        "            error_msg = f'‚ùå Failed to load model with {attn_impl} attention: {str(e)}'\n",
        "            raise RuntimeError(error_msg)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model.eval()\n",
        "        self.num_layers = len(self.model.model.layers)\n",
        "\n",
        "    # NO CHANGE to get_residual_stream, it is correct.\n",
        "    def get_residual_stream(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                           past_key_values=None, use_cache: bool = True) -> Tuple[torch.Tensor, torch.Tensor, Optional[List]]:\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            residual_stream = outputs.hidden_states[-1]\n",
        "            new_past_key_values = outputs.past_key_values if use_cache else None\n",
        "            return logits, residual_stream, new_past_key_values\n",
        "\n",
        "    # BUG FIX 1: Added position_ids to the layer call.\n",
        "    def second_pass_forward(self, residual_stream: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                      num_layers: Optional[int] = None) -> torch.Tensor:\n",
        "      with torch.no_grad():\n",
        "        hidden_states = self.model.model.norm(residual_stream) if self.config.residual_variant == 'layernorm' and hasattr(self.model.model, 'norm') else residual_stream\n",
        "        layers_to_use = min(num_layers or self.num_layers, self.num_layers)\n",
        "\n",
        "        # Create position_ids based on the sequence length of the residual stream\n",
        "        seq_len = residual_stream.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)\n",
        "\n",
        "        # The model expects a 4D attention mask for the layers. We can create it from the 2D mask.\n",
        "        # This is often handled internally, but being explicit is safer in custom loops.\n",
        "        causal_mask = self.model._prepare_decoder_attention_mask(\n",
        "            attention_mask, (1, seq_len), hidden_states, 0\n",
        "        )\n",
        "\n",
        "        for i in range(layers_to_use):\n",
        "            outputs = self.model.model.layers[i](\n",
        "                hidden_states,\n",
        "                attention_mask=causal_mask,\n",
        "                position_ids=position_ids, # Pass the position IDs\n",
        "                use_cache=False\n",
        "            )\n",
        "            hidden_states = outputs[0]\n",
        "\n",
        "        hidden_states = self.model.model.norm(hidden_states) if hasattr(self.model.model, 'norm') else hidden_states\n",
        "        return self.model.lm_head(hidden_states)\n",
        "\n",
        "    # BUG FIX 2: Completely rewritten generate loop for correct state management.\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 100) -> Dict:\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True).to(self.device)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        generated_token_ids = input_ids\n",
        "        entropies1, entropies2 = [], []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # --- Pass 1: Standard autoregressive step to get the next token logits ---\n",
        "            # For the first pass, we can use the efficient KV cache.\n",
        "            # We pass the *entire* generated sequence so far. The model's use_cache\n",
        "            # logic will handle only processing the last token.\n",
        "            outputs = self.model(\n",
        "                input_ids=generated_token_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                use_cache=True, # Note: This will be overwritten if past_key_values is passed\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "\n",
        "            # The logits for the VERY LAST token are what we need to predict the next one.\n",
        "            logits1 = outputs.logits[:, -1, :]\n",
        "\n",
        "            probs1 = F.softmax(logits1, dim=-1)\n",
        "            entropy1 = -torch.sum(probs1 * torch.log(probs1 + self.config.entropy_eps), dim=-1).item()\n",
        "            entropies1.append(entropy1)\n",
        "\n",
        "            final_logits = logits1\n",
        "            entropies2.append(entropy1) # Default for baseline\n",
        "\n",
        "            # --- Pass 2: The \"Double Pass\" logic ---\n",
        "            # This is only performed if not in baseline mode.\n",
        "            if self.config.pass_type != 'baseline':\n",
        "                # To get the full residual stream, we MUST run a forward pass\n",
        "                # on the entire sequence WITHOUT the KV cache. This is inefficient\n",
        "                # but is the only way to correctly implement the experimental idea.\n",
        "                with torch.no_grad():\n",
        "                    full_outputs = self.model(\n",
        "                        input_ids=generated_token_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        output_hidden_states=True,\n",
        "                        use_cache=False # CRITICAL: No cache to get the full residual\n",
        "                    )\n",
        "\n",
        "                full_residual_stream = full_outputs.hidden_states[-1]\n",
        "\n",
        "                num_l = self.config.second_pass_layers if self.config.pass_type == 'double_partial' else None\n",
        "                logits2 = self.second_pass_forward(full_residual_stream, attention_mask, num_l)\n",
        "\n",
        "                final_logits = logits2[:, -1, :] # We only care about the last token's logits\n",
        "\n",
        "                probs2 = F.softmax(final_logits, dim=-1)\n",
        "                entropy2 = -torch.sum(probs2 * torch.log(probs2 + self.config.entropy_eps), dim=-1).item()\n",
        "                entropies2[-1] = entropy2 # Overwrite the last entropy value\n",
        "\n",
        "\n",
        "            # --- Sampling ---\n",
        "            probs = F.softmax(final_logits / self.config.temperature, dim=-1)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > self.config.top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            probs[indices_to_remove] = 0.0\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Append the new token for the next iteration\n",
        "            generated_token_ids = torch.cat([generated_token_ids, next_token], dim=-1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones((1,1), device=self.device)], dim=-1)\n",
        "\n",
        "\n",
        "        generated_text = self.tokenizer.decode(generated_token_ids[0], skip_special_tokens=True)\n",
        "        # We need to remove the original prompt from the output\n",
        "        if generated_text.startswith(prompt):\n",
        "            generated_text = generated_text[len(prompt):]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'generated_text': generated_text.strip(),\n",
        "            'entropies1': entropies1,\n",
        "            'entropies2': entropies2,\n",
        "            'num_tokens': len(entropies1),\n",
        "            'time': time.time() - start_time\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "0f2610380be84937830be2989461ac98",
            "cdf1ca8b58d64ad9832f951784b556c5",
            "5ab33e308f794ad7be9d7a929ad82c95",
            "2f0ca6d3a98f4c3f9f136edb8d50e074",
            "6931c202840b41fdaebb28945cebb20b",
            "1679ecc5b61746398bdfde59a5afc693",
            "9d9d4672ed8844c5b13cf80067034be2",
            "c19cb2afe952494fad7f0735d54ad82b",
            "0f778984e039426eb96f7707b3c1637a",
            "bd8f4cfe25664d7eb6d6be7ddfc6acd8",
            "7ea2c3d250744ec985aad0d2a10de5c1"
          ]
        },
        "id": "QWPFy9zOdGI8",
        "outputId": "ddb8b6ff-c4d4-44ac-8793-fdbf1567a2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f2610380be84937830be2989461ac98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully with eager attention\n",
            "Smoke test passed: the size of the dataset should be divisible by in 1.05s\n"
          ]
        }
      ],
      "source": [
        "### 5Ô∏è‚É£ Quick Smoke Test\n",
        "\n",
        "try:\n",
        "    model = DoublePassPhi3(config.model_path, config)\n",
        "    result = model.generate('Sanity check:', max_new_tokens=10)\n",
        "    print(f'Smoke test passed: {result[\"generated_text\"]} in {result[\"time\"]:.2f}s')\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f'Smoke test failed: {str(e)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xtfXJmZPdGI8"
      },
      "outputs": [],
      "source": [
        "### 6Ô∏è‚É£ Simple Benchmark System\n",
        "\n",
        "# Simple Q&A dataset for testing\n",
        "SIMPLE_QA_DATASET = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
        "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
        "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"Shakespeare\"},\n",
        "    {\"question\": \"What is the square root of 16?\", \"answer\": \"4\"},\n",
        "    {\"question\": \"What year did World War II end?\", \"answer\": \"1945\"},\n",
        "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"},\n",
        "    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo\"},\n",
        "    {\"question\": \"How many days are in a leap year?\", \"answer\": \"366\"},\n",
        "    {\"question\": \"What is the speed of light?\", \"answer\": \"299,792,458\"},\n",
        "]\n",
        "\n",
        "def simple_benchmark(model: DoublePassPhi3, dataset: List[Dict] = None) -> Dict:\n",
        "    \"\"\"Run a simple Q&A benchmark.\"\"\"\n",
        "    if dataset is None:\n",
        "        dataset = SIMPLE_QA_DATASET\n",
        "\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "    results = []\n",
        "    total_time = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item[\"question\"]\n",
        "        expected = item[\"answer\"].lower()\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = model.generate(question, max_new_tokens=20)\n",
        "        inference_time = time.time() - start_time\n",
        "        total_time += inference_time\n",
        "\n",
        "        generated = result['generated_text'].lower().strip()\n",
        "\n",
        "        # Simple scoring - check if expected answer is in generated text\n",
        "        is_correct = expected in generated or any(word in generated for word in expected.split())\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected,\n",
        "            'generated': result['generated_text'],\n",
        "            'correct': is_correct,\n",
        "            'time': inference_time\n",
        "        })\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_time = total_time / total\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_time': avg_time,\n",
        "        'total_time': total_time,\n",
        "        'samples': total,\n",
        "        'results': results\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9TglC5GbdGI8"
      },
      "outputs": [],
      "source": [
        "### 7Ô∏è‚É£ Simple Benchmark Runner\n",
        "\n",
        "def run_simple_benchmark(model: DoublePassPhi3) -> Dict:\n",
        "    \"\"\"Run the simple Q&A benchmark.\"\"\"\n",
        "    try:\n",
        "        print(f\"Running simple benchmark with {model.config.pass_type} method...\")\n",
        "        results = simple_benchmark(model)\n",
        "        print(f\"‚úÖ Completed: {results['accuracy']:.2f} accuracy, {results['avg_time']:.2f}s avg time\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error in benchmark: {str(e)}')\n",
        "        return {'accuracy': 0.0, 'samples': 0, 'avg_time': 0.0, 'total_time': 0.0}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simple_benchmark(DoublePassPhi3(config.model_path, config))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "76c6e2963a304671a7a24c50e2873223",
            "4e1cc5cea20d4d0b82783c130738e010",
            "92807af59c5d41f0ac17a029190a1896",
            "285faa59d1dd4584bbd18f5f932d68d6",
            "a0b96b4207a44d0c950b4579960db9b5",
            "49947eab2cde4330a0198eb2d8b32209",
            "2ca4c1dff5df493aba7f32054bdc0fb7",
            "85c59e5ce0de4666b3ac0089624a3bd6",
            "0ed34f27280346cca8dd1064f3b01499",
            "fb994974b9d84940a000d70b380d4dd7",
            "f5f8a1b1a65c46a689be04573ac93e27"
          ]
        },
        "collapsed": true,
        "id": "bzOmAuk5t1xl",
        "outputId": "c74b8a93-5f2d-4ff0-8554-97827294e102"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76c6e2963a304671a7a24c50e2873223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully with eager attention\n",
            "Running simple benchmark with baseline method...\n",
            "‚úÖ Completed: 0.80 accuracy, 1.04s avg time\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8,\n",
              " 'avg_time': 1.0375317096710206,\n",
              " 'total_time': 10.375317096710205,\n",
              " 'samples': 10,\n",
              " 'results': [{'question': 'What is the capital of France?',\n",
              "   'expected': 'paris',\n",
              "   'generated': '# Answer\\nThe capital of France is Paris.',\n",
              "   'correct': True,\n",
              "   'time': 0.7548317909240723},\n",
              "  {'question': 'What is 2 + 2?',\n",
              "   'expected': '4',\n",
              "   'generated': \"I know this one, it's 4. Now, can you give me a harder question\",\n",
              "   'correct': True,\n",
              "   'time': 1.1072328090667725},\n",
              "  {'question': 'What is the largest planet in our solar system?',\n",
              "   'expected': 'jupiter',\n",
              "   'generated': 'Jupiter is the largest planet in our solar system. It is a gas giant and is known for',\n",
              "   'correct': True,\n",
              "   'time': 1.1078200340270996},\n",
              "  {'question': 'Who wrote Romeo and Juliet?',\n",
              "   'expected': 'shakespeare',\n",
              "   'generated': '# Answer\\nWilliam Shakespeare wrote \"Romeo and Juliet.\"',\n",
              "   'correct': True,\n",
              "   'time': 0.984142541885376},\n",
              "  {'question': 'What is the square root of 16?',\n",
              "   'expected': '4',\n",
              "   'generated': '# Answer\\nThe square root of 16 is 4.',\n",
              "   'correct': True,\n",
              "   'time': 0.9916331768035889},\n",
              "  {'question': 'What year did World War II end?',\n",
              "   'expected': '1945',\n",
              "   'generated': 'Answer: World War II ended in the year 1945. This was a',\n",
              "   'correct': True,\n",
              "   'time': 1.088244915008545},\n",
              "  {'question': 'What is the chemical symbol for gold?',\n",
              "   'expected': 'au',\n",
              "   'generated': 'Gold\\'s chemical symbol is Au, derived from the Latin word \"aurum.\"',\n",
              "   'correct': True,\n",
              "   'time': 1.085278034210205},\n",
              "  {'question': 'What is the capital of Japan?',\n",
              "   'expected': 'tokyo',\n",
              "   'generated': 'The capital of Japan is Tokyo. Tokyo is the political, economic, and cultural center of Japan.',\n",
              "   'correct': True,\n",
              "   'time': 1.087764024734497},\n",
              "  {'question': 'How many days are in a leap year?',\n",
              "   'expected': '366',\n",
              "   'generated': 'To determine the number of days in a leap year, we need to know the difference between a',\n",
              "   'correct': False,\n",
              "   'time': 1.090019702911377},\n",
              "  {'question': 'What is the speed of light?',\n",
              "   'expected': '299,792,458',\n",
              "   'generated': 'I need the speed of light in a vacuum. Please provide the value in meters',\n",
              "   'correct': False,\n",
              "   'time': 1.0783500671386719}]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NbkyKqgYdGI9"
      },
      "outputs": [],
      "source": [
        "### 8Ô∏è‚É£ Single Experiment Driver\n",
        "\n",
        "def run_experiment(config: ExperimentConfig) -> Dict:\n",
        "    \"\"\"Run a single experiment with the given configuration.\"\"\"\n",
        "    print(f\"\\nüöÄ Starting experiment: {config.pass_type}\")\n",
        "\n",
        "    model = DoublePassPhi3(config.model_path, config)\n",
        "    results = run_simple_benchmark(model)\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    path = Path(f'/content/drive/MyDrive/runs/{timestamp}-{config.pass_type}.json')\n",
        "    path.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "    print(f'üìä Results: Accuracy {results[\"accuracy\"]:.3f}, Avg Time {results[\"avg_time\"]:.2f}s')\n",
        "    print(f'üíæ Saved to: {path}')\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LJuGxrDtdGI9"
      },
      "outputs": [],
      "source": [
        "### 9Ô∏è‚É£ Grid Search Launcher\n",
        "\n",
        "\n",
        "# Programmatic experiment grid generation\n",
        "def generate_experiment_configs(custom_experiments: List[Dict] = None) -> List[ExperimentConfig]:\n",
        "    \"\"\"Generate list of experiment configurations. Optionally add custom experiments.\"\"\"\n",
        "    experiments = []\n",
        "\n",
        "    # Standard experiment set\n",
        "    experiments.append(ExperimentConfig(pass_type='baseline', residual_variant='raw'))\n",
        "    experiments.append(ExperimentConfig(pass_type='double_full', residual_variant='raw'))\n",
        "    experiments.append(ExperimentConfig(pass_type='double_full', residual_variant='layernorm'))\n",
        "\n",
        "    # Systematic partial pass experiments\n",
        "    for layers in [1, 2, 4, 8]:\n",
        "        experiments.append(ExperimentConfig(\n",
        "            pass_type='double_partial',\n",
        "            second_pass_layers=layers,\n",
        "            residual_variant='raw'\n",
        "        ))\n",
        "\n",
        "    # Add custom experiments if provided\n",
        "    if custom_experiments:\n",
        "        for custom in custom_experiments:\n",
        "            experiments.append(ExperimentConfig(**custom))\n",
        "\n",
        "    return experiments\n",
        "\n",
        "def run_all_experiments(custom_experiments: List[Dict] = None):\n",
        "    \"\"\"Run all experiments and compare results.\"\"\"\n",
        "    exps = generate_experiment_configs(custom_experiments)\n",
        "    print(f'üî¨ Running {len(exps)} experiments, estimated time: {len(exps)*5:.0f} minutes')\n",
        "\n",
        "    all_results = []\n",
        "    for i, exp in enumerate(exps, 1):\n",
        "        print(f\"\\nüìä Experiment {i}/{len(exps)}\")\n",
        "        try:\n",
        "            res = run_experiment(exp)\n",
        "            variant_name = f\"{exp.pass_type}_{exp.residual_variant}\"\n",
        "            if exp.second_pass_layers:\n",
        "                variant_name += f\"_L{exp.second_pass_layers}\"\n",
        "            all_results.append((variant_name, res['accuracy']))\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed experiment {exp.pass_type}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Sort by accuracy\n",
        "    all_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print('\\nüèÜ LEADERBOARD:')\n",
        "    print('=' * 40)\n",
        "    for rank, (name, acc) in enumerate(all_results, 1):\n",
        "        print(f'{rank}. {name}: {acc:.3f}')\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "10A4G5HXdGI9"
      },
      "outputs": [],
      "source": [
        "### üîü Interactive Playground & Quick Test\n",
        "\n",
        "def playground():\n",
        "    \"\"\"Interactive playground to compare baseline vs double pass.\"\"\"\n",
        "    print(\"üéÆ Interactive Playground - Compare Baseline vs Double Pass\")\n",
        "    print(\"Type 'q' to quit, or enter any prompt to test both methods.\\n\")\n",
        "\n",
        "    while True:\n",
        "        prompt = input('üìù Enter prompt (or q to quit): ')\n",
        "        if prompt.lower() == 'q': break\n",
        "\n",
        "        print(f\"\\nüîç Testing prompt: '{prompt}'\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Baseline experiment\n",
        "        print(\"üöÄ Running baseline...\")\n",
        "        base_config = ExperimentConfig(pass_type='baseline')\n",
        "        base_model = DoublePassPhi3(base_config.model_path, base_config)\n",
        "        base_res = base_model.generate(prompt, max_new_tokens=30)\n",
        "        del base_model\n",
        "\n",
        "        # Double pass experiment\n",
        "        print(\"üöÄ Running double pass...\")\n",
        "        double_config = ExperimentConfig(pass_type='double_full')\n",
        "        double_model = DoublePassPhi3(double_config.model_path, double_config)\n",
        "        double_res = double_model.generate(prompt, max_new_tokens=30)\n",
        "        del double_model\n",
        "\n",
        "        # Clear CUDA cache to prevent memory leak\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate entropy differences\n",
        "        base_ent = sum(base_res['entropies1']) / len(base_res['entropies1']) if base_res['entropies1'] else 0\n",
        "        double_ent = sum(double_res['entropies2']) / len(double_res['entropies2']) if double_res['entropies2'] else 0\n",
        "\n",
        "        print(f\"\\nüìä RESULTS:\")\n",
        "        print(f\"üîµ Baseline: {base_res['generated_text']}\")\n",
        "        print(f\"   Time: {base_res['time']:.2f}s, Entropy: {base_ent:.3f}\")\n",
        "        print(f\"üü£ Double Pass: {double_res['generated_text']}\")\n",
        "        print(f\"   Time: {double_res['time']:.2f}s, Entropy: {double_ent:.3f}\")\n",
        "        print(f\"‚ö° Speed ratio: {double_res['time']/base_res['time']:.1f}x slower\")\n",
        "        print(f\"üß† Entropy change: {base_ent - double_ent:.3f}\\n\")\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Run a quick test of both methods.\"\"\"\n",
        "    print(\"üß™ Quick Test - Baseline vs Double Pass\\n\")\n",
        "\n",
        "    test_prompts = [\n",
        "        \"The capital of France is\",\n",
        "        \"2 + 2 equals\",\n",
        "        \"The largest planet is\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"üîç Testing: '{prompt}'\")\n",
        "\n",
        "        # Baseline\n",
        "        base_config = ExperimentConfig(pass_type='baseline')\n",
        "        base_model = DoublePassPhi3(base_config.model_path, base_config)\n",
        "        base_res = base_model.generate(prompt, max_new_tokens=10)\n",
        "        del base_model\n",
        "\n",
        "        # Double pass\n",
        "        double_config = ExperimentConfig(pass_type='double_full')\n",
        "        double_model = DoublePassPhi3(double_config.model_path, double_config)\n",
        "        double_res = double_model.generate(prompt, max_new_tokens=10)\n",
        "        del double_model\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"  üîµ Baseline: {base_res['generated_text']}\")\n",
        "        print(f\"  üü£ Double: {double_res['generated_text']}\")\n",
        "        print(f\"  ‚ö° Speed: {double_res['time']/base_res['time']:.1f}x slower\\n\")\n",
        "\n",
        "# Uncomment to run:\n",
        "# playground()\n",
        "# quick_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: GSM8K BENCHMARKING SYSTEM\n",
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "def parse_gsm8k_answer(generated_text: str) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Parses the model's generated text to find the final numerical answer for GSM8K.\n",
        "    It looks for the '####' marker or the last number in the string.\n",
        "    This version is more robust against malformed numbers.\n",
        "    \"\"\"\n",
        "    # Look for the #### pattern, which is standard for GSM8K reasoning chains\n",
        "    match = re.search(r\"####\\s*([0-9,.]+)\", generated_text)\n",
        "    if match:\n",
        "        # Extract number, remove commas, and convert to float\n",
        "        num_str = match.group(1).replace(',', '')\n",
        "        if num_str:\n",
        "            try:\n",
        "                return float(num_str)\n",
        "            except ValueError:\n",
        "                # The match after #### was not a valid number, so we continue to the fallback\n",
        "                pass\n",
        "\n",
        "    # If #### is not found, fall back to finding the last valid number in the string.\n",
        "    matches = re.findall(r\"([0-9,.]+)\", generated_text)\n",
        "    if not matches:\n",
        "        return None # Return None if no numbers are found at all\n",
        "\n",
        "    # Iterate backwards through all found matches\n",
        "    for match in reversed(matches):\n",
        "        # Clean the string (remove commas)\n",
        "        cleaned_match = match.replace(',', '')\n",
        "\n",
        "        # Guard against empty strings or strings that are just a period\n",
        "        if cleaned_match and cleaned_match != '.':\n",
        "            try:\n",
        "                # Try to convert to a float\n",
        "                return float(cleaned_match)\n",
        "            except ValueError:\n",
        "                # If this match fails (e.g., it was \"1.2.3\"), ignore it and continue to the next one\n",
        "                continue\n",
        "\n",
        "    # If no valid number was found in any of the matches\n",
        "    return None\n",
        "\n",
        "def run_gsm8k_benchmark(model: DoublePassPhi3, num_questions: int = 100) -> Dict:\n",
        "    \"\"\"\n",
        "    Runs a benchmark on the GSM8K dataset.\n",
        "\n",
        "    Args:\n",
        "        model: An initialized DoublePassPhi3 model.\n",
        "        num_questions: The number of questions to run from the test set.\n",
        "                       Set to 'all' to run the entire benchmark (takes a long time!).\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Loading GSM8K dataset...\")\n",
        "    try:\n",
        "        # Load the 'main' configuration of gsm8k\n",
        "        dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load dataset: {e}\")\n",
        "        return {}\n",
        "\n",
        "    if num_questions != 'all':\n",
        "        dataset = dataset.select(range(num_questions))\n",
        "        print(f\"üìä Running on the first {num_questions} questions of the GSM8K test set.\")\n",
        "    else:\n",
        "        print(f\"üìä Running on the ENTIRE {len(dataset)} questions of the GSM8K test set. This will take a while!\")\n",
        "\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "    results = []\n",
        "    total_time = 0\n",
        "\n",
        "    for i, item in enumerate(dataset):\n",
        "        question = item[\"question\"]\n",
        "        # The expected answer is just the number\n",
        "        expected_answer_text = item[\"answer\"].split(\"####\")[-1].strip()\n",
        "        expected = float(expected_answer_text.replace(',', ''))\n",
        "\n",
        "        # Generate an answer. We need more tokens for GSM8K's reasoning.\n",
        "        result = model.generate(question, max_new_tokens=256)\n",
        "        total_time += result['time']\n",
        "\n",
        "        # Parse the final number from the model's output\n",
        "        generated = parse_gsm8k_answer(result['generated_text'])\n",
        "        is_correct = generated is not None and generated == expected\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        print(f\"  Q{i+1}/{total}: Expected: {expected}, Got: {generated} -> {'‚úÖ Correct' if is_correct else '‚ùå Incorrect'}\")\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected,\n",
        "            'generated_text': result['generated_text'],\n",
        "            'parsed_answer': generated,\n",
        "            'correct': is_correct,\n",
        "            'time': result['time']\n",
        "        })\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    avg_time = total_time / total if total > 0 else 0\n",
        "\n",
        "    benchmark_results = {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_time': avg_time,\n",
        "        'total_time': total_time,\n",
        "        'samples': total,\n",
        "        'pass_type': model.config.pass_type,\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    print(f\"\\n‚úÖ GSM8K BENCHMARK COMPLETE ({model.config.pass_type})\")\n",
        "    print(f\"   Accuracy: {accuracy:.3f} ({correct}/{total})\")\n",
        "    print(f\"   Average Time per Question: {avg_time:.2f}s\")\n",
        "\n",
        "    return benchmark_results"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T30HFuZ7tD3l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to run the GSM8K benchmark comparison\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set the number of questions you want to test.\n",
        "# Use a small number like 40 for a quick test.\n",
        "# Use 'all' for the full benchmark (can take hours).\n",
        "NUM_QUESTIONS_TO_RUN = 40\n",
        "\n",
        "# --- 1. RUN BASELINE MODEL ---\n",
        "print(\"=\"*60)\n",
        "print(\"üìä STARTING GSM8K BENCHMARK: BASELINE\")\n",
        "print(\"=\"*60)\n",
        "# Create a config for the baseline run\n",
        "baseline_config = ExperimentConfig(pass_type='baseline')\n",
        "# Initialize the model with this config\n",
        "baseline_model = DoublePassPhi3(baseline_config.model_path, baseline_config)\n",
        "# Run the benchmark\n",
        "baseline_results = run_gsm8k_benchmark(baseline_model, num_questions=NUM_QUESTIONS_TO_RUN)\n",
        "# Clean up memory\n",
        "del baseline_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 2. RUN DOUBLE PASS MODEL ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä STARTING GSM8K BENCHMARK: DOUBLE PASS (FULL)\")\n",
        "print(\"=\"*60)\n",
        "# Create a config for your experimental run\n",
        "# We will test a full double pass here\n",
        "double_pass_config = ExperimentConfig(\n",
        "    pass_type='double_full',\n",
        "    residual_variant='raw' # or 'layernorm'\n",
        ")\n",
        "# Initialize the model\n",
        "double_pass_model = DoublePassPhi3(double_pass_config.model_path, double_pass_config)\n",
        "# Run the benchmark\n",
        "double_pass_results = run_gsm8k_benchmark(double_pass_model, num_questions=NUM_QUESTIONS_TO_RUN)\n",
        "# Clean up memory\n",
        "del double_pass_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 3. COMPARE RESULTS ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ FINAL GSM8K BENCHMARK COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "if baseline_results:\n",
        "    print(f\"üîµ Baseline Accuracy: {baseline_results['accuracy']:.3f}\")\n",
        "    print(f\"   Avg. Time: {baseline_results['avg_time']:.2f}s\")\n",
        "if double_pass_results:\n",
        "    print(f\"üü£ Double Pass Accuracy: {double_pass_results['accuracy']:.3f}\")\n",
        "    print(f\"   Avg. Time: {double_pass_results['avg_time']:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d3ad6682f1fe4a169b3541867f5845d5",
            "d7bc11d3938a4232bcb233387b9baced",
            "16a69595679f4528aac8219007ec27c7",
            "236f8f4575ac4ac29a623861cb75658c",
            "a7920f02be454807bf156185f0787680",
            "14d7e8b8378e46a8815e2cc0f6419d59",
            "fad5d8fbcc0b4f738f90e648e12dfde5",
            "bdeb7ebc37d749ada084af7e6148f226",
            "e87c1a0d88a444b1b919485d300afca8",
            "9f3ec780bd5545e0ad9c697fa842522a",
            "9cef4501b7474eec8b6f3e39b29b7723",
            "4ff0682898214e87a3366b5dc6cd2757",
            "78a8a784c5ce410fa25ddcf494ab07b9",
            "6b087b0fb62841aaa7ba49f0b43dd4c2",
            "cad61c65ffdc4b1f92b0634dc41a65ec",
            "f0bd3755aa0849d283ee4b7b9e77cf1f",
            "e8d7c5c1fb274bd1969fc845d530da0f",
            "86d2aa4531f14d029f70ce607ca5c1a9",
            "70c9120737584f658a0057d131085761",
            "2828427083cc48f39c3d792ef5bbfaff",
            "f11baa72462741668c27341f052d4b45",
            "691656c8b4f54a7cbc063e3175f56d92"
          ]
        },
        "id": "nOfjvEXO5Eh7",
        "outputId": "73c62e33-f37b-4e8e-94e9-9a6131f9ad7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üìä STARTING GSM8K BENCHMARK: BASELINE\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3ad6682f1fe4a169b3541867f5845d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully with eager attention\n",
            "üöÄ Loading GSM8K dataset...\n",
            "üìä Running on the first 20 questions of the GSM8K test set.\n",
            "  Q1/20: Expected: 18.0, Got: 18.0 -> ‚úÖ Correct\n",
            "  Q2/20: Expected: 3.0, Got: 3.0 -> ‚úÖ Correct\n",
            "  Q3/20: Expected: 70000.0, Got: 195000.0 -> ‚ùå Incorrect\n",
            "  Q4/20: Expected: 540.0, Got: 540.0 -> ‚úÖ Correct\n",
            "  Q5/20: Expected: 20.0, Got: 140.0 -> ‚ùå Incorrect\n",
            "  Q6/20: Expected: 64.0, Got: 16.0 -> ‚ùå Incorrect\n",
            "  Q7/20: Expected: 260.0, Got: 260.0 -> ‚úÖ Correct\n",
            "  Q8/20: Expected: 160.0, Got: 5.0 -> ‚ùå Incorrect\n",
            "  Q9/20: Expected: 45.0, Got: 135.0 -> ‚ùå Incorrect\n",
            "  Q10/20: Expected: 460.0, Got: 60.0 -> ‚ùå Incorrect\n",
            "  Q11/20: Expected: 366.0, Got: 366.0 -> ‚úÖ Correct\n",
            "  Q12/20: Expected: 694.0, Got: 694.0 -> ‚úÖ Correct\n",
            "  Q13/20: Expected: 13.0, Got: 12.0 -> ‚ùå Incorrect\n",
            "  Q14/20: Expected: 18.0, Got: 5.0 -> ‚ùå Incorrect\n",
            "  Q15/20: Expected: 60.0, Got: 60.0 -> ‚úÖ Correct\n",
            "  Q16/20: Expected: 125.0, Got: 5000.0 -> ‚ùå Incorrect\n",
            "  Q17/20: Expected: 230.0, Got: 230.0 -> ‚úÖ Correct\n",
            "  Q18/20: Expected: 57500.0, Got: 35000.0 -> ‚ùå Incorrect\n",
            "  Q19/20: Expected: 7.0, Got: 7.0 -> ‚úÖ Correct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.phi3_3.8B.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Q20/20: Expected: 6.0, Got: 1.5 -> ‚ùå Incorrect\n",
            "\n",
            "‚úÖ GSM8K BENCHMARK COMPLETE (baseline)\n",
            "   Accuracy: 0.450 (9/20)\n",
            "   Average Time per Question: 12.37s\n",
            "\n",
            "============================================================\n",
            "üìä STARTING GSM8K BENCHMARK: DOUBLE PASS (FULL)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ff0682898214e87a3366b5dc6cd2757"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully with eager attention\n",
            "üöÄ Loading GSM8K dataset...\n",
            "üìä Running on the first 20 questions of the GSM8K test set.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Phi3ForCausalLM' object has no attribute '_prepare_decoder_attention_mask'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3511375206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdouble_pass_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoublePassPhi3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdouble_pass_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_pass_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Run the benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdouble_pass_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_gsm8k_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdouble_pass_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_questions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_QUESTIONS_TO_RUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m# Clean up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdouble_pass_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-1721231334.py\u001b[0m in \u001b[0;36mrun_gsm8k_benchmark\u001b[0;34m(model, num_questions)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Generate an answer. We need more tokens for GSM8K's reasoning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2000129072.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mnum_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_pass_layers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'double_partial'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_pass_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_residual_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mfinal_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# We only care about the last token's logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2000129072.py\u001b[0m in \u001b[0;36msecond_pass_forward\u001b[0;34m(self, residual_stream, attention_mask, num_layers)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# The model expects a 4D attention mask for the layers. We can create it from the 2D mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# This is often handled internally, but being explicit is safer in custom loops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         causal_mask = self.model._prepare_decoder_attention_mask(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Phi3ForCausalLM' object has no attribute '_prepare_decoder_attention_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxo68mIudGI9"
      },
      "outputs": [],
      "source": [
        "### 1Ô∏è‚É£1Ô∏è‚É£ Results Analysis & Plots\n",
        "\n",
        "def analyze_results(runs_dir='/content/drive/MyDrive/runs'):\n",
        "    \"\"\"Analyze experiment results with visualization.\"\"\"\n",
        "    files = glob(f'{runs_dir}/*.json')\n",
        "    if not files:\n",
        "        print('No results found');\n",
        "        return\n",
        "\n",
        "    data = []\n",
        "    for f in files:\n",
        "        with open(f) as jf:\n",
        "            res = json.load(jf)\n",
        "            variant = Path(f).stem.split('-')[1]\n",
        "            for bench, metrics in res.items():\n",
        "                data.append({\n",
        "                    'variant': variant,\n",
        "                    'benchmark': bench,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'runtime': metrics['runtime']\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "    # Average accuracy by variant\n",
        "    df.groupby('variant')['accuracy'].mean().sort_values(ascending=False).plot.bar(ax=axs[0])\n",
        "    axs[0].set_title('Avg Accuracy by Variant')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "\n",
        "    # Accuracy vs Runtime scatter\n",
        "    df.plot.scatter(x='runtime', y='accuracy', ax=axs[1])\n",
        "    axs[1].set_title('Accuracy vs Runtime')\n",
        "    axs[1].set_xlabel('Runtime (seconds)')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{runs_dir}/analysis.png')\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "# analyze_results()  # Uncomment to run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9J9O0pTdGI9"
      },
      "outputs": [],
      "source": [
        "### 1Ô∏è‚É£2Ô∏è‚É£ Usage Examples & How to Run\n",
        "\n",
        "print(\"‚úÖ All functions loaded successfully!\")\n",
        "print(\"\\nüöÄ HOW TO RUN THE EXPERIMENT:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ QUICK START:\")\n",
        "print(\"   quick_test()  # Run quick comparison\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ SINGLE EXPERIMENTS:\")\n",
        "print(\"   # Baseline method\")\n",
        "print(\"   baseline_config = ExperimentConfig(pass_type='baseline')\")\n",
        "print(\"   result1 = run_experiment(baseline_config)\")\n",
        "print(\"\")\n",
        "print(\"   # Double pass method\")\n",
        "print(\"   double_config = ExperimentConfig(pass_type='double_full')\")\n",
        "print(\"   result2 = run_experiment(double_config)\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ FULL COMPARISON:\")\n",
        "print(\"   run_all_experiments()  # Compare all methods\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ INTERACTIVE MODE:\")\n",
        "print(\"   playground()  # Test custom prompts\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ CUSTOM EXPERIMENTS:\")\n",
        "print(\"   custom_exps = [\")\n",
        "print(\"       {'pass_type': 'double_partial', 'second_pass_layers': 4}\")\n",
        "print(\"   ]\")\n",
        "print(\"   run_all_experiments(custom_experiments=custom_exps)\")\n",
        "\n",
        "print(\"\\nüí° TIPS:\")\n",
        "print(\"- Run smoke test first to verify model loading\")\n",
        "print(\"- Start with quick_test() for immediate results\")\n",
        "print(\"- Each experiment takes ~2-5 minutes\")\n",
        "print(\"- Results saved to /content/drive/MyDrive/runs/\")\n",
        "print(\"- Use Ctrl+C to stop long-running experiments\")\n",
        "\n",
        "print(\"\\nüéØ RECOMMENDED WORKFLOW:\")\n",
        "print(\"1. Run the smoke test (cell 5)\")\n",
        "print(\"2. Try quick_test() for immediate comparison\")\n",
        "print(\"3. Run single experiments to understand differences\")\n",
        "print(\"4. Use playground() to test specific prompts\")\n",
        "print(\"5. Run full comparison with run_all_experiments()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJwt6aQpdGI9"
      },
      "outputs": [],
      "source": [
        "# Additional utility functions and debug helpers\n",
        "\n",
        "def print_model_summary(model):\n",
        "    \"\"\"Print model architecture summary.\"\"\"\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(f\"Device: {model.device}\")\n",
        "    print(f\"Layers: {model.num_layers}\")\n",
        "    print(f\"Config: {model.config}\")\n",
        "\n",
        "# Quick profiling: torch.utils.bottleneck.run(model.generate('test'))\n",
        "# Toggle attention: config.attn_impl = 'eager'\n",
        "# Memory check: print(f\"VRAM used: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_rDStfJdGI-"
      },
      "outputs": [],
      "source": [
        "### 1Ô∏è‚É£4Ô∏è‚É£ Cleanup & Tips\n",
        "\n",
        "\n",
        "# Tips:\n",
        "# - Restart runtime if OOM: Runtime > Restart session\n",
        "# - Download results: Files > Mount Drive > Copy from /content/drive/MyDrive/runs\n",
        "# - Expected times: Baseline ~30min/benchmark, Double ~60min\n",
        "# - Checklist: Run smoke test, check GPU, verify model path, start with single exp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGjnf9gvdGI-"
      },
      "outputs": [],
      "source": [
        "### 1Ô∏è‚É£5Ô∏è‚É£ Ready to Run! üöÄ\n",
        "\n",
        "print(\"üéâ SETUP COMPLETE!\")\n",
        "print(\"All functions are loaded and ready to use.\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ TO START EXPERIMENTING, RUN ONE OF THESE:\")\n",
        "print(\"=\"*60)\n",
        "print(\"üëâ quick_test()           # 2-minute quick demo\")\n",
        "print(\"üëâ playground()           # Interactive testing\")\n",
        "print(\"üëâ run_all_experiments()  # Full comparison (~30 min)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüí° WHAT EACH METHOD DOES:\")\n",
        "print(\"üîµ Baseline: Normal single forward pass (standard AI)\")\n",
        "print(\"üü£ Double Pass: Runs model twice for each word (experimental)\")\n",
        "print(\"\\nüéØ GOAL: See if 'thinking twice' improves accuracy!\")\n",
        "\n",
        "# Uncomment any of these to run:\n",
        "# quick_test()\n",
        "# playground()\n",
        "# run_all_experiments()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f2610380be84937830be2989461ac98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf1ca8b58d64ad9832f951784b556c5",
              "IPY_MODEL_5ab33e308f794ad7be9d7a929ad82c95",
              "IPY_MODEL_2f0ca6d3a98f4c3f9f136edb8d50e074"
            ],
            "layout": "IPY_MODEL_6931c202840b41fdaebb28945cebb20b"
          }
        },
        "cdf1ca8b58d64ad9832f951784b556c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1679ecc5b61746398bdfde59a5afc693",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9d9d4672ed8844c5b13cf80067034be2",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "5ab33e308f794ad7be9d7a929ad82c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19cb2afe952494fad7f0735d54ad82b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f778984e039426eb96f7707b3c1637a",
            "value": 2
          }
        },
        "2f0ca6d3a98f4c3f9f136edb8d50e074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8f4cfe25664d7eb6d6be7ddfc6acd8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7ea2c3d250744ec985aad0d2a10de5c1",
            "value": "‚Äá2/2‚Äá[00:06&lt;00:00,‚Äá‚Äá2.92s/it]"
          }
        },
        "6931c202840b41fdaebb28945cebb20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1679ecc5b61746398bdfde59a5afc693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9d4672ed8844c5b13cf80067034be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c19cb2afe952494fad7f0735d54ad82b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f778984e039426eb96f7707b3c1637a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd8f4cfe25664d7eb6d6be7ddfc6acd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea2c3d250744ec985aad0d2a10de5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c6e2963a304671a7a24c50e2873223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e1cc5cea20d4d0b82783c130738e010",
              "IPY_MODEL_92807af59c5d41f0ac17a029190a1896",
              "IPY_MODEL_285faa59d1dd4584bbd18f5f932d68d6"
            ],
            "layout": "IPY_MODEL_a0b96b4207a44d0c950b4579960db9b5"
          }
        },
        "4e1cc5cea20d4d0b82783c130738e010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49947eab2cde4330a0198eb2d8b32209",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ca4c1dff5df493aba7f32054bdc0fb7",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "92807af59c5d41f0ac17a029190a1896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c59e5ce0de4666b3ac0089624a3bd6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ed34f27280346cca8dd1064f3b01499",
            "value": 2
          }
        },
        "285faa59d1dd4584bbd18f5f932d68d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb994974b9d84940a000d70b380d4dd7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f5f8a1b1a65c46a689be04573ac93e27",
            "value": "‚Äá2/2‚Äá[00:06&lt;00:00,‚Äá‚Äá3.03s/it]"
          }
        },
        "a0b96b4207a44d0c950b4579960db9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49947eab2cde4330a0198eb2d8b32209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca4c1dff5df493aba7f32054bdc0fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85c59e5ce0de4666b3ac0089624a3bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed34f27280346cca8dd1064f3b01499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb994974b9d84940a000d70b380d4dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f8a1b1a65c46a689be04573ac93e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3ad6682f1fe4a169b3541867f5845d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7bc11d3938a4232bcb233387b9baced",
              "IPY_MODEL_16a69595679f4528aac8219007ec27c7",
              "IPY_MODEL_236f8f4575ac4ac29a623861cb75658c"
            ],
            "layout": "IPY_MODEL_a7920f02be454807bf156185f0787680"
          }
        },
        "d7bc11d3938a4232bcb233387b9baced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14d7e8b8378e46a8815e2cc0f6419d59",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fad5d8fbcc0b4f738f90e648e12dfde5",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "16a69595679f4528aac8219007ec27c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdeb7ebc37d749ada084af7e6148f226",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e87c1a0d88a444b1b919485d300afca8",
            "value": 2
          }
        },
        "236f8f4575ac4ac29a623861cb75658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3ec780bd5545e0ad9c697fa842522a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9cef4501b7474eec8b6f3e39b29b7723",
            "value": "‚Äá2/2‚Äá[00:06&lt;00:00,‚Äá‚Äá3.01s/it]"
          }
        },
        "a7920f02be454807bf156185f0787680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14d7e8b8378e46a8815e2cc0f6419d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad5d8fbcc0b4f738f90e648e12dfde5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdeb7ebc37d749ada084af7e6148f226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e87c1a0d88a444b1b919485d300afca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f3ec780bd5545e0ad9c697fa842522a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cef4501b7474eec8b6f3e39b29b7723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ff0682898214e87a3366b5dc6cd2757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78a8a784c5ce410fa25ddcf494ab07b9",
              "IPY_MODEL_6b087b0fb62841aaa7ba49f0b43dd4c2",
              "IPY_MODEL_cad61c65ffdc4b1f92b0634dc41a65ec"
            ],
            "layout": "IPY_MODEL_f0bd3755aa0849d283ee4b7b9e77cf1f"
          }
        },
        "78a8a784c5ce410fa25ddcf494ab07b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8d7c5c1fb274bd1969fc845d530da0f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_86d2aa4531f14d029f70ce607ca5c1a9",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "6b087b0fb62841aaa7ba49f0b43dd4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c9120737584f658a0057d131085761",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2828427083cc48f39c3d792ef5bbfaff",
            "value": 2
          }
        },
        "cad61c65ffdc4b1f92b0634dc41a65ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f11baa72462741668c27341f052d4b45",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_691656c8b4f54a7cbc063e3175f56d92",
            "value": "‚Äá2/2‚Äá[00:06&lt;00:00,‚Äá‚Äá2.96s/it]"
          }
        },
        "f0bd3755aa0849d283ee4b7b9e77cf1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d7c5c1fb274bd1969fc845d530da0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d2aa4531f14d029f70ce607ca5c1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70c9120737584f658a0057d131085761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2828427083cc48f39c3d792ef5bbfaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f11baa72462741668c27341f052d4b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "691656c8b4f54a7cbc063e3175f56d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}